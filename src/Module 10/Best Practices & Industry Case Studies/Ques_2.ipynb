{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ML Model Monitoring Pipelines\n",
    "\n",
    "### Model Performance Drift:\n",
    "**Description**: Setup a monitoring pipeline to track key performance metrics (e.g., accuracy, precision) of an ML model over time using a monitoring tool or dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution Drift:\n",
    "**Description**: Monitor the distribution of your input features in deployed models to detect any significant shifts from training data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection in Predictions:\n",
    "**DEscription**: Implement an anomaly detection mechanism to flag unusual model\n",
    "predictions. Simulate anomalies by altering input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'great_expectations' has no attribute 'DataContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize Great Expectations Data Context\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mgx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataContext\u001b[49m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- Model Performance Drift ---\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Model Performance Drift ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'great_expectations' has no attribute 'DataContext'"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Initialize Great Expectations Data Context\n",
    "context = gx.DataContext()\n",
    "\n",
    "# --- Model Performance Drift ---\n",
    "print(\"\\n--- Model Performance Drift ---\")\n",
    "# Simulate historical model performance data (replace with your actual monitoring data)\n",
    "historical_performance = [\n",
    "    {\"date\": \"2025-05-01\", \"accuracy\": 0.85, \"precision\": 0.78},\n",
    "    {\"date\": \"2025-05-08\", \"accuracy\": 0.86, \"precision\": 0.79},\n",
    "    {\"date\": \"2025-05-15\", \"accuracy\": 0.82, \"precision\": 0.75},\n",
    "]\n",
    "performance_df = pd.DataFrame(historical_performance)\n",
    "performance_df['date'] = pd.to_datetime(performance_df['date'])\n",
    "\n",
    "# Add Pandas DataFrame Data Source and Data Asset\n",
    "datasource_name = \"model_performance_source\"\n",
    "datasource = context.sources.add_pandas(name=datasource_name)\n",
    "data_asset_name = \"model_performance\"\n",
    "data_asset = datasource.add_dataframe_asset(name=data_asset_name)\n",
    "batch_request = data_asset.build_batch_request(dataframe=performance_df)\n",
    "\n",
    "# Get Validator\n",
    "validator_performance = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"model_performance_drift_suite\",\n",
    ")\n",
    "\n",
    "print(f\"Using Expectation Suite: {validator_performance.expectation_suite.name}\")\n",
    "\n",
    "# Define expectations for performance metrics (example: accuracy should not drop significantly)\n",
    "validator_performance.expect_column_mean_to_be_between(\n",
    "    column=\"accuracy\",\n",
    "    min_value=0.80,\n",
    "    max_value=1.0,\n",
    "    mostly=0.9,\n",
    ")\n",
    "validator_performance.expect_column_mean_to_be_between(\n",
    "    column=\"precision\",\n",
    "    min_value=0.75,\n",
    "    max_value=1.0,\n",
    "    mostly=0.9,\n",
    ")\n",
    "\n",
    "validator_performance.save_expectation_suite()\n",
    "\n",
    "checkpoint_performance = context.run_checkpoint(\n",
    "    checkpoint_name=\"model_performance_drift_checkpoint\",\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": batch_request,\n",
    "            \"expectation_suite_name\": \"model_performance_drift_suite\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(\"Model Performance Drift Validation Results:\")\n",
    "print(checkpoint_performance.list_validation_results())\n",
    "\n",
    "# --- Feature Distribution Drift ---\n",
    "print(\"\\n--- Feature Distribution Drift ---\")\n",
    "# Simulate training and deployed data (replace with your actual data)\n",
    "train_data = pd.DataFrame({'feature_a': np.random.normal(0, 1, 1000)})\n",
    "deployed_data = pd.DataFrame({'feature_a': np.random.normal(0.5, 1.2, 1000)})\n",
    "\n",
    "# Add Data Sources and Assets\n",
    "datasource_name = \"feature_drift_source\"\n",
    "datasource = context.sources.add_pandas(name=datasource_name)\n",
    "\n",
    "train_asset = datasource.add_dataframe_asset(name=\"train_data\")\n",
    "train_batch_request = train_asset.build_batch_request(dataframe=train_data)\n",
    "\n",
    "deployed_asset = datasource.add_dataframe_asset(name=\"deployed_data\")\n",
    "deployed_batch_request = deployed_asset.build_batch_request(dataframe=deployed_data)\n",
    "\n",
    "# Get Validators\n",
    "validator_train = context.get_validator(\n",
    "    batch_request=train_batch_request,\n",
    "    expectation_suite_name=\"feature_drift_train_suite\",\n",
    ")\n",
    "validator_deployed = context.get_validator(\n",
    "    batch_request=deployed_batch_request,\n",
    "    expectation_suite_name=\"feature_drift_deployed_suite\",\n",
    ")\n",
    "\n",
    "print(f\"Using Expectation Suite (Train): {validator_train.expectation_suite.name}\")\n",
    "print(f\"Using Expectation Suite (Deployed): {validator_deployed.expectation_suite.name}\")\n",
    "\n",
    "# Define expectation to compare distributions using Kolmogorov-Smirnov test (custom expectation)\n",
    "def expect_kolmogorov_smirnov_p_value_greater_than(self, column, other_batch_request, threshold=0.05):\n",
    "    batch = self.get_batch(batch_request=self._active_batch_request)\n",
    "    other_batch = self.get_batch(batch_request=other_batch_request)\n",
    "    data1 = batch.data[column].dropna()\n",
    "    data2 = other_batch.data[column].dropna()\n",
    "    if len(data1) < 2 or len(data2) < 2:\n",
    "        return self.expectation_failed(\n",
    "            details={\"reason\": \"Insufficient data for KS test\"}\n",
    "        )\n",
    "    ks_statistic, p_value = ks_2samp(data1, data2)\n",
    "    success = p_value > threshold\n",
    "    return self.expectation_met(\n",
    "        success=success,\n",
    "        result={\"statistic\": ks_statistic, \"pvalue\": p_value, \"threshold\": threshold},\n",
    "    )\n",
    "\n",
    "gx.validator.ExpectationSuite.expect_kolmogorov_smirnov_p_value_greater_than = expect_kolmogorov_smirnov_p_value_greater_than\n",
    "\n",
    "validator_deployed.expect_kolmogorov_smirnov_p_value_greater_than(\n",
    "    column=\"feature_a\",\n",
    "    other_batch_request=train_batch_request,\n",
    "    threshold=0.05,\n",
    ")\n",
    "\n",
    "validator_deployed.save_expectation_suite()\n",
    "\n",
    "checkpoint_feature_drift = context.run_checkpoint(\n",
    "    checkpoint_name=\"feature_drift_checkpoint\",\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": deployed_batch_request,\n",
    "            \"expectation_suite_name\": \"feature_drift_deployed_suite\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(\"Feature Distribution Drift Validation Results:\")\n",
    "print(checkpoint_feature_drift.list_validation_results())\n",
    "\n",
    "# --- Anomaly Detection in Predictions ---\n",
    "print(\"\\n--- Anomaly Detection in Predictions ---\")\n",
    "# Simulate model predictions (replace with your actual predictions)\n",
    "predictions = pd.DataFrame({'prediction': np.concatenate([np.random.normal(5, 1, 950), np.random.normal(10, 2, 50)])})\n",
    "\n",
    "# Add Data Source and Data Asset\n",
    "datasource_name = \"prediction_anomaly_source\"\n",
    "datasource = context.sources.add_pandas(name=datasource_name)\n",
    "data_asset_name = \"model_predictions\"\n",
    "data_asset = datasource.add_dataframe_asset(name=data_asset_name)\n",
    "batch_request = data_asset.build_batch_request(dataframe=predictions)\n",
    "\n",
    "# Get Validator\n",
    "validator_anomaly = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"prediction_anomaly_suite\",\n",
    ")\n",
    "\n",
    "print(f\"Using Expectation Suite: {validator_anomaly.expectation_suite.name}\")\n",
    "\n",
    "# Use Isolation Forest to detect anomalies (outside of Great Expectations for detection)\n",
    "model = IsolationForest(contamination=0.05)\n",
    "outliers = model.fit_predict(predictions[['prediction']])\n",
    "anomalous_predictions = predictions[outliers == -1]\n",
    "non_anomalous_predictions = predictions[outliers != -1]\n",
    "\n",
    "print(f\"Number of anomalous predictions: {len(anomalous_predictions)}\")\n",
    "print(f\"Example anomalous predictions:\\n{anomalous_predictions.head()}\")\n",
    "\n",
    "# Define expectations on the range of predictions (to catch gross anomalies)\n",
    "validator_anomaly.expect_column_min_to_be_greater_than_or_equal_to(column=\"prediction\", min_value=0)\n",
    "validator_anomaly.expect_column_max_to_be_less_than_or_equal_to(column=\"prediction\", max_value=15)\n",
    "validator_anomaly.expect_column_values_to_be_in_type_list(column=\"prediction\", type_list=[\"float\", \"int\"])\n",
    "\n",
    "# You could also check the percentage of predictions falling outside a typical range\n",
    "lower_bound = non_anomalous_predictions['prediction'].quantile(0.01)\n",
    "upper_bound = non_anomalous_predictions['prediction'].quantile(0.99)\n",
    "validator_anomaly.expect_column_values_to_be_between(\n",
    "    column=\"prediction\",\n",
    "    min_value=lower_bound,\n",
    "    max_value=upper_bound,\n",
    "    mostly=0.99,\n",
    "    meta={\"notes\": \"Checking if 99% of predictions fall within the 1st and 99th percentile of non-anomalous data\"},\n",
    ")\n",
    "\n",
    "validator_anomaly.save_expectation_suite()\n",
    "\n",
    "checkpoint_anomaly = context.run_checkpoint(\n",
    "    checkpoint_name=\"prediction_anomaly_checkpoint\",\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": batch_request,\n",
    "            \"expectation_suite_name\": \"prediction_anomaly_suite\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(\"Anomaly Detection in Predictions Validation Results:\")\n",
    "print(checkpoint_anomaly.list_validation_results())\n",
    "\n",
    "print(\"\\nTo view detailed validation reports, run 'great_expectations docs build' in your project directory and open the generated index.html.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

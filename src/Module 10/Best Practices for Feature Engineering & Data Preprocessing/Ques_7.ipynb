{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Task 1: Consistent Feature Preparation ---\n",
      "Training Data:\n",
      "   numerical_col categorical_col\n",
      "0            1.0               A\n",
      "1            2.0            None\n",
      "2            NaN               B\n",
      "3            4.0               A\n",
      "4            5.0               C\n",
      "\n",
      "Inference Data:\n",
      "   numerical_col categorical_col\n",
      "0            6.0               B\n",
      "1            NaN               A\n",
      "2            8.0            None\n",
      "3            9.0               C\n",
      "4           10.0               B\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Apply the consistent preprocessing function to both datasets\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m train_df_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m inference_df_processed \u001b[38;5;241m=\u001b[39m preprocess_data(inference_df\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessed Training Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[1;32m     37\u001b[0m     imputer_categorical \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m imputer_categorical\u001b[38;5;241m.\u001b[39mfit_transform(df[[col]])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4300\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4292\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4293\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4300\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4303\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4306\u001b[0m     ):\n\u001b[1;32m   4307\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/frame.py:5040\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   5039\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m-> 5040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/construction.py:608\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    606\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 608\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    610\u001b[0m         object_index\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    613\u001b[0m     ):\n\u001b[1;32m    614\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1172\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(value))  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;66;03m# Caller is responsible\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(value\u001b[38;5;241m.\u001b[39mndim)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# --- Task 1: Consistent Feature Preparation ---\n",
    "print(\"--- Task 1: Consistent Feature Preparation ---\")\n",
    "\n",
    "# Create sample training and inference datasets\n",
    "train_data = {'numerical_col': [1, 2, None, 4, 5],\n",
    "              'categorical_col': ['A', None, 'B', 'A', 'C']}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "inference_data = {'numerical_col': [6, None, 8, 9, 10],\n",
    "                  'categorical_col': ['B', 'A', None, 'C', 'B']}\n",
    "inference_df = pd.DataFrame(inference_data)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(train_df)\n",
    "print(\"\\nInference Data:\")\n",
    "print(inference_df)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Function for preprocessing data, including imputation.\n",
    "    \"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        imputer_numerical = SimpleImputer(strategy='mean')\n",
    "        df[col] = imputer_numerical.fit_transform(df[[col]])\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "        df[col] = imputer_categorical.fit_transform(df[[col]])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the consistent preprocessing function to both datasets\n",
    "train_df_processed = preprocess_data(train_df.copy())\n",
    "inference_df_processed = preprocess_data(inference_df.copy())\n",
    "\n",
    "print(\"\\nProcessed Training Data:\")\n",
    "print(train_df_processed)\n",
    "print(\"\\nProcessed Inference Data:\")\n",
    "print(inference_df_processed)\n",
    "\n",
    "print(\"\\nThis demonstrates consistent application of the same preprocessing steps (mean imputation for numerical, mode for categorical) to both training and inference datasets, ensuring feature consistency.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 2: Pipeline Integration ---\n",
    "print(\"--- Task 2: Pipeline Integration ---\")\n",
    "\n",
    "# Create sample training and inference datasets (with a target variable for training)\n",
    "train_data_pipeline = {'numerical_col': [1, 2, None, 4, 5],\n",
    "                       'categorical_col': ['A', None, 'B', 'A', 'C'],\n",
    "                       'target': [0, 1, 0, 1, 0]}\n",
    "train_df_pipeline = pd.DataFrame(train_data_pipeline)\n",
    "\n",
    "inference_data_pipeline = {'numerical_col': [6, None, 8, 9, 10],\n",
    "                           'categorical_col': ['B', 'A', None, 'C', 'B']}\n",
    "inference_df_pipeline = pd.DataFrame(inference_data_pipeline)\n",
    "\n",
    "X_train = train_df_pipeline.drop('target', axis=1)\n",
    "y_train = train_df_pipeline['target']\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols_pipeline = X_train.select_dtypes(include=['number']).columns\n",
    "categorical_cols_pipeline = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', pd.get_dummies, {'handle_unknown': 'ignore'}) # handle_unknown for inference\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols_pipeline),\n",
    "        ('cat', categorical_transformer, categorical_cols_pipeline)\n",
    "    ])\n",
    "\n",
    "# Create the full training pipeline with a model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "training_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('classifier', LogisticRegression())])\n",
    "\n",
    "# Train the pipeline\n",
    "training_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Configure an identical preprocessing pipeline for inference\n",
    "inference_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Apply the preprocessing pipeline to the inference data\n",
    "inference_df_processed_pipeline = inference_pipeline.transform(inference_df_pipeline)\n",
    "\n",
    "print(\"\\nProcessed Inference Data using Pipeline:\")\n",
    "print(inference_df_processed_pipeline)\n",
    "\n",
    "print(\"\\nUsing scikit-learn pipelines encapsulates all preprocessing steps, ensuring that the exact same transformations are applied to both training and inference data. The `ColumnTransformer` helps apply different transformations to different columns.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 3: Saving and Loading Preprocessing Models ---\n",
    "print(\"--- Task 3: Saving and Loading Preprocessing Models ---\")\n",
    "\n",
    "# Create a simpler preprocessing pipeline (just numerical scaling for demonstration)\n",
    "numerical_data_for_save = pd.DataFrame({'feature1': [1, 2, 3, 4, 5],\n",
    "                                        'feature2': [5, 4, None, 2, 1]})\n",
    "\n",
    "numerical_processor_save = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit the preprocessing pipeline to the training data\n",
    "numerical_processor_save.fit(numerical_data_for_save)\n",
    "\n",
    "# Save the fitted preprocessing model\n",
    "preprocessing_model_path = 'preprocessing_model.joblib'\n",
    "joblib.dump(numerical_processor_save, preprocessing_model_path)\n",
    "print(f\"Preprocessing model saved to: {preprocessing_model_path}\")\n",
    "\n",
    "# Load the saved preprocessing model during inference\n",
    "loaded_preprocessor = joblib.load(preprocessing_model_path)\n",
    "\n",
    "# Create new inference data\n",
    "inference_data_for_load = pd.DataFrame({'feature1': [6, 7, 8, None, 10],\n",
    "                                         'feature2': [1, 3, 2, 4, None]})\n",
    "\n",
    "# Apply the loaded preprocessing model to the inference data\n",
    "inference_data_processed_loaded = loaded_preprocessor.transform(inference_data_for_load)\n",
    "\n",
    "print(\"\\nInference Data before loading and applying preprocessor:\")\n",
    "print(inference_data_for_load)\n",
    "print(\"\\nInference Data after loading and applying the saved preprocessor:\")\n",
    "print(inference_data_processed_loaded)\n",
    "\n",
    "print(\"\\nSaving the fitted preprocessing model (e.g., using `joblib`) allows you to reuse the exact same transformation learned from the training data during inference, ensuring consistency without retraining the preprocessor on the inference data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

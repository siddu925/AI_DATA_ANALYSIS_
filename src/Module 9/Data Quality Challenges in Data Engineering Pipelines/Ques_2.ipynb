{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Handling Schema Mismatches using Spark\n",
    "**Description**: Use Apache Spark to address schema mismatches by transforming data to match\n",
    "the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Create Spark session\n",
    "2. Load dataframe\n",
    "3. Define the expected schema\n",
    "4. Handle schema mismatches\n",
    "5. Show corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write your code from here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, StringType, IntegerType, BooleanType, FloatType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, lit\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, FloatType\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Step 1: Create Spark session\n",
    "spark = SparkSession.builder.appName(\"HandleSchemaMismatches\").getOrCreate()\n",
    "\n",
    "# Sample data with potential schema mismatches (imagine this comes from a file)\n",
    "data_mismatched = [\n",
    "    (\"Alice\", \"30\", \"true\", \"60000.5\"),\n",
    "    (\"Bob\", 25, \"false\", \"55000\"),\n",
    "    (\"Charlie\", \"40\", 1, 70000.0),\n",
    "    (None, \"28\", \"TRUE\", \"62000.75\"),\n",
    "]\n",
    "\n",
    "# Define the schema of the mismatched data (as Spark might infer it)\n",
    "schema_mismatched = [\"name\", \"age_str\", \"is_active_str_int\", \"salary_str\"]\n",
    "df_mismatched = spark.createDataFrame(data_mismatched, schema_mismatched)\n",
    "\n",
    "# Step 2: Load dataframe (already done above for demonstration)\n",
    "print(\"DataFrame with potential schema mismatches:\")\n",
    "df_mismatched.show()\n",
    "df_mismatched.printSchema()\n",
    "\n",
    "# Step 3: Define the expected schema\n",
    "expected_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"salary\", FloatType(), True)\n",
    "])\n",
    "\n",
    "print(\"\\nExpected Schema:\")\n",
    "expected_schema.printTreeString()\n",
    "\n",
    "# Step 4: Handle schema mismatches\n",
    "df_corrected = df_mismatched.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age_str\").cast(IntegerType()).alias(\"age\"),\n",
    "    (col(\"is_active_str_int\") == \"true\").cast(BooleanType()).alias(\"is_active\"),\n",
    "    col(\"salary_str\").cast(FloatType()).alias(\"salary\")\n",
    ")\n",
    "\n",
    "# Handle potential null values or parsing errors more robustly\n",
    "df_corrected = df_mismatched.select(\n",
    "    col(\"name\"),\n",
    "    (col(\"age_str\").cast(IntegerType())).alias(\"age\"),\n",
    "    (\n",
    "        (col(\"is_active_str_int\") == \"true\") | (col(\"is_active_str_int\") == \"TRUE\") | (col(\"is_active_str_int\") == 1)\n",
    "    ).cast(BooleanType()).alias(\"is_active\"),\n",
    "    (col(\"salary_str\").cast(FloatType())).alias(\"salary\")\n",
    ")\n",
    "\n",
    "# Add a new column if it's missing in the source\n",
    "if \"city_str\" not in df_mismatched.columns:\n",
    "    df_with_city = df_mismatched.withColumn(\"city\", lit(None).cast(StringType()))\n",
    "else:\n",
    "    df_with_city = df_mismatched.withColumnRenamed(\"city_str\", \"city\")\n",
    "\n",
    "# Select and cast columns to match the expected schema, handling missing columns\n",
    "df_corrected = df_with_city.select(\n",
    "    col(\"name\"),\n",
    "    (col(\"age_str\").cast(IntegerType())).alias(\"age\"),\n",
    "    (\n",
    "        (col(\"is_active_str_int\") == \"true\") | (col(\"is_active_str_int\") == \"TRUE\") | (col(\"is_active_str_int\") == 1)\n",
    "    ).cast(BooleanType()).alias(\"is_active\"),\n",
    "    (col(\"salary_str\").cast(FloatType())).alias(\"salary\"),\n",
    "    col(\"city\")\n",
    ")\n",
    "\n",
    "# Ensure the final DataFrame has exactly the columns of the expected schema (and in the right order if needed)\n",
    "expected_columns = [field.name for field in expected_schema]\n",
    "if \"city\" in df_corrected.columns and \"city\" not in expected_columns:\n",
    "    expected_schema_with_city = StructType(expected_schema.fields + [StructField(\"city\", StringType(), True)])\n",
    "    expected_columns = [field.name for field in expected_schema_with_city]\n",
    "\n",
    "final_df = df_corrected.select(*expected_columns)\n",
    "\n",
    "# Step 5: Show corrected data\n",
    "print(\"\\nCorrected DataFrame matching the expected schema:\")\n",
    "final_df.show()\n",
    "final_df.printSchema()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Detect and Correct Incomplete Data in ETL\n",
    "**Description**: Use Python and Pandas to detect incomplete data in an ETL process and fill\n",
    "missing values with estimates.\n",
    "\n",
    "**Steps**:\n",
    "1. Detect incomplete data\n",
    "2. Fill missing values\n",
    "3. Report changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from: your_data_with_missing.csv\n",
      "\n",
      "Initial missing values per column:\n",
      "col1    1\n",
      "col2    2\n",
      "col3    1\n",
      "col4    2\n",
      "dtype: int64\n",
      "\n",
      "Total initial missing values: 6\n",
      "\n",
      "Missing values per column after imputation:\n",
      "col3    1\n",
      "dtype: int64\n",
      "\n",
      "Total missing values after imputation: 1\n",
      "\n",
      "Number of missing values filled: 5\n",
      "\n",
      "DataFrame after mean imputation:\n",
      "   col1      col2 col3      col4\n",
      "0   1.0  7.333333    A  1.100000\n",
      "1   2.0  6.000000    B  2.933333\n",
      "2   3.0  7.000000  NaN  3.300000\n",
      "3   4.0  7.333333    A  4.400000\n",
      "4   5.0  9.000000    C  2.933333\n",
      "Data loaded successfully from: your_data_with_missing.csv\n",
      "\n",
      "Initial missing values per column:\n",
      "col1    1\n",
      "col2    2\n",
      "col3    1\n",
      "col4    2\n",
      "dtype: int64\n",
      "\n",
      "Total initial missing values: 6\n",
      "\n",
      "Missing values per column after imputation:\n",
      "col2    2\n",
      "col3    1\n",
      "dtype: int64\n",
      "\n",
      "Total missing values after imputation: 3\n",
      "\n",
      "Number of missing values filled: 3\n",
      "\n",
      "DataFrame after median imputation (for col1 and col4):\n",
      "   col1  col2 col3  col4\n",
      "0   1.0   NaN    A   1.1\n",
      "1   2.0   6.0    B   3.3\n",
      "2   3.0   7.0  NaN   3.3\n",
      "3   4.0   NaN    A   4.4\n",
      "4   5.0   9.0    C   3.3\n",
      "Data loaded successfully from: your_data_with_missing.csv\n",
      "\n",
      "Initial missing values per column:\n",
      "col1    1\n",
      "col2    2\n",
      "col3    1\n",
      "col4    2\n",
      "dtype: int64\n",
      "\n",
      "Total initial missing values: 6\n",
      "\n",
      "Missing values per column after imputation:\n",
      "col1    1\n",
      "col2    2\n",
      "col4    2\n",
      "dtype: int64\n",
      "\n",
      "Total missing values after imputation: 5\n",
      "\n",
      "Number of missing values filled: 1\n",
      "\n",
      "DataFrame after mode imputation (for col3):\n",
      "   col1  col2 col3  col4\n",
      "0   1.0   NaN    A   1.1\n",
      "1   2.0   6.0    B   NaN\n",
      "2   NaN   7.0    A   3.3\n",
      "3   4.0   NaN    A   4.4\n",
      "4   5.0   9.0    C   NaN\n",
      "Data loaded successfully from: your_data_with_missing.csv\n",
      "\n",
      "Initial missing values per column:\n",
      "col1    1\n",
      "col2    2\n",
      "col3    1\n",
      "col4    2\n",
      "dtype: int64\n",
      "\n",
      "Total initial missing values: 6\n",
      "\n",
      "Missing values per column after imputation:\n",
      "col1    1\n",
      "col2    2\n",
      "col4    2\n",
      "dtype: int64\n",
      "\n",
      "Total missing values after imputation: 5\n",
      "\n",
      "Number of missing values filled: 1\n",
      "\n",
      "DataFrame after constant imputation (for col3):\n",
      "   col1  col2     col3  col4\n",
      "0   1.0   NaN        A   1.1\n",
      "1   2.0   6.0        B   NaN\n",
      "2   NaN   7.0  Missing   3.3\n",
      "3   4.0   NaN        A   4.4\n",
      "4   5.0   9.0        C   NaN\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "\n",
    "def handle_incomplete_data(file_path, imputation_strategy='mean', fill_value=None, columns_to_impute=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, detects incomplete (missing) data,\n",
    "    fills missing values using a specified strategy, and reports the changes.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        imputation_strategy (str, optional): The strategy to use for filling missing values.\n",
    "                                             Options: 'mean', 'median', 'mode', 'constant'.\n",
    "                                             Defaults to 'mean'.\n",
    "        fill_value (any, optional): The constant value to use if imputation_strategy is 'constant'.\n",
    "                                    Defaults to None.\n",
    "        columns_to_impute (list, optional): A list of column names to apply imputation to.\n",
    "                                           If None, imputation will be applied to all columns\n",
    "                                           with missing values that support the chosen strategy.\n",
    "                                           Defaults to None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Detect incomplete data\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully from: {file_path}\\n\")\n",
    "\n",
    "        initial_missing_values = df.isnull().sum()\n",
    "        print(\"Initial missing values per column:\")\n",
    "        print(initial_missing_values[initial_missing_values > 0])\n",
    "        initial_total_missing = initial_missing_values.sum()\n",
    "        print(f\"\\nTotal initial missing values: {initial_total_missing}\")\n",
    "\n",
    "        df_before_imputation = df.copy()\n",
    "\n",
    "        # Step 2: Fill missing values\n",
    "        if imputation_strategy == 'mean':\n",
    "            if columns_to_impute:\n",
    "                for col in columns_to_impute:\n",
    "                    if df[col].dtype in ['int64', 'float64']:\n",
    "                        df[col].fillna(df[col].mean(), inplace=True)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot apply mean imputation to non-numeric column '{col}'. Skipping.\")\n",
    "            else:\n",
    "                for col in df.columns:\n",
    "                    if df[col].dtype in ['int64', 'float64']:\n",
    "                        df[col].fillna(df[col].mean(), inplace=True)\n",
    "        elif imputation_strategy == 'median':\n",
    "            if columns_to_impute:\n",
    "                for col in columns_to_impute:\n",
    "                    if df[col].dtype in ['int64', 'float64']:\n",
    "                        df[col].fillna(df[col].median(), inplace=True)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot apply median imputation to non-numeric column '{col}'. Skipping.\")\n",
    "            else:\n",
    "                for col in df.columns:\n",
    "                    if df[col].dtype in ['int64', 'float64']:\n",
    "                        df[col].fillna(df[col].median(), inplace=True)\n",
    "        elif imputation_strategy == 'mode':\n",
    "            if columns_to_impute:\n",
    "                for col in columns_to_impute:\n",
    "                    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                for col in df.columns:\n",
    "                    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        elif imputation_strategy == 'constant':\n",
    "            if fill_value is not None:\n",
    "                if columns_to_impute:\n",
    "                    for col in columns_to_impute:\n",
    "                        df[col].fillna(fill_value, inplace=True)\n",
    "                else:\n",
    "                    df.fillna(fill_value, inplace=True)\n",
    "            else:\n",
    "                print(\"Error: 'fill_value' must be specified when using 'constant' imputation.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error: Invalid imputation strategy '{imputation_strategy}'. Options are: 'mean', 'median', 'mode', 'constant'.\")\n",
    "            return None\n",
    "\n",
    "        # Step 3: Report changes\n",
    "        final_missing_values = df.isnull().sum()\n",
    "        print(\"\\nMissing values per column after imputation:\")\n",
    "        print(final_missing_values[final_missing_values > 0])\n",
    "        final_total_missing = final_missing_values.sum()\n",
    "        print(f\"\\nTotal missing values after imputation: {final_total_missing}\")\n",
    "\n",
    "        num_filled = initial_total_missing - final_total_missing\n",
    "        print(f\"\\nNumber of missing values filled: {num_filled}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example Usage:\n",
    "file_path = 'your_data_with_missing.csv'  # Replace with the actual path\n",
    "\n",
    "# Create a sample CSV file with missing values for testing (optional)\n",
    "data = {'col1': [1, 2, None, 4, 5],\n",
    "        'col2': [None, 6, 7, None, 9],\n",
    "        'col3': ['A', 'B', None, 'A', 'C'],\n",
    "        'col4': [1.1, None, 3.3, 4.4, None]}\n",
    "df_sample = pd.DataFrame(data)\n",
    "df_sample.to_csv(file_path, index=False)\n",
    "\n",
    "# Handle incomplete data using mean imputation for numeric columns\n",
    "df_filled_mean = handle_incomplete_data(file_path)\n",
    "if df_filled_mean is not None:\n",
    "    print(\"\\nDataFrame after mean imputation:\")\n",
    "    print(df_filled_mean)\n",
    "\n",
    "# Handle incomplete data using median imputation for specific numeric columns\n",
    "df_filled_median = handle_incomplete_data(file_path, imputation_strategy='median', columns_to_impute=['col1', 'col4'])\n",
    "if df_filled_median is not None:\n",
    "    print(\"\\nDataFrame after median imputation (for col1 and col4):\")\n",
    "    print(df_filled_median)\n",
    "\n",
    "# Handle incomplete data using mode imputation for a categorical column\n",
    "df_filled_mode = handle_incomplete_data(file_path, imputation_strategy='mode', columns_to_impute=['col3'])\n",
    "if df_filled_mode is not None:\n",
    "    print(\"\\nDataFrame after mode imputation (for col3):\")\n",
    "    print(df_filled_mode)\n",
    "\n",
    "# Handle incomplete data using constant value imputation\n",
    "df_filled_constant = handle_incomplete_data(file_path, imputation_strategy='constant', fill_value='Missing', columns_to_impute=['col3'])\n",
    "if df_filled_constant is not None:\n",
    "    print(\"\\nDataFrame after constant imputation (for col3):\")\n",
    "    print(df_filled_constant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

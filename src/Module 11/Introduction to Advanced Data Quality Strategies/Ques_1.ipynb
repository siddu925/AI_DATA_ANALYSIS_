{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Profiling to Understand Data Quality\n",
    "**Description**: Use basic statistical methods to profile a dataset and identify potential quality issues.\n",
    "\n",
    "**Steps**:\n",
    "1. Load the dataset using pandas in Python.\n",
    "2. Understand the data by checking its basic statistics.\n",
    "3. Identify null values.\n",
    "4. Check unique values for categorical columns.\n",
    "5. Review outliers using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement Simple Data Validation\n",
    "**Description**: Write a Python script to validate the data types and constraints of each column in a dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Define constraints for each column.\n",
    "2. Validate each column based on its constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Detect Missing Data Patterns\n",
    "**Description**: Analyze and visualize missing data patterns in a dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Visualize missing data using a heatmap.\n",
    "2. Identify patterns in missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Integrate Automated Data Quality Checks\n",
    "**Description**: Integrate automated data quality checks using the Great Expectations library for a dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Install and initialize Great Expectations.\n",
    "2. Set up Great Expectations.\n",
    "3. Add further checks and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'your_dataset.csv' was not found. Please make sure the file is in the correct directory.\n",
      "\n",
      "Basic statistics of the dataset:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 2. Basic statistics\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBasic statistics of the dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 3. Identify null values\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNull values in each column:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Task 1: Data Profiling to Understand Data Quality\n",
    "# Description: Use basic statistical methods to profile a dataset and identify potential quality issues.\n",
    "# Steps:\n",
    "# 1. Load the dataset using pandas in Python.\n",
    "# 2. Understand the data by checking its basic statistics.\n",
    "# 3. Identify null values.\n",
    "# 4. Check unique values for categorical columns.\n",
    "# 5. Review outliers using box plots.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with the actual file name\n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'your_dataset.csv' was not found. Please make sure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Basic statistics\n",
    "print(\"\\nBasic statistics of the dataset:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# 3. Identify null values\n",
    "print(\"\\nNull values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 4. Check unique values for categorical columns\n",
    "print(\"\\nUnique values for categorical columns:\")\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    print(f\"\\nColumn: {column}\")\n",
    "    print(df[column].unique())\n",
    "    print(f\"Number of unique values: {df[column].nunique()}\")\n",
    "\n",
    "# 5. Review outliers using box plots\n",
    "print(\"\\nBox plots to visualize outliers:\")\n",
    "for column in df.select_dtypes(include=np.number).columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=df[column])\n",
    "    plt.title(f'Box plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.show()\n",
    "\n",
    "# Task 2: Implement Simple Data Validation\n",
    "# Description: Write a Python script to validate the data types and constraints of each column in a dataset.\n",
    "# Steps:\n",
    "# 1. Define constraints for each column.\n",
    "# 2. Validate each column based on its constraints.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the same DataFrame 'df' from Task 1\n",
    "\n",
    "# 1. Define constraints for each column\n",
    "constraints = {\n",
    "    'column_name_1': {'dtype': 'int64', 'min': 0, 'max': 100},\n",
    "    'column_name_2': {'dtype': 'object', 'allowed_values': ['A', 'B', 'C']},\n",
    "    'column_name_3': {'dtype': 'float64', 'min': 1.0, 'max': 5.0},\n",
    "    # Add constraints for other columns as needed\n",
    "}\n",
    "\n",
    "# 2. Validate each column based on its constraints\n",
    "def validate_data(df, constraints):\n",
    "    validation_errors = {}\n",
    "    for column, rules in constraints.items():\n",
    "        if column not in df.columns:\n",
    "            validation_errors[column] = \"Column not found\"\n",
    "            continue\n",
    "\n",
    "        errors = []\n",
    "        if 'dtype' in rules:\n",
    "            if df[column].dtype != rules['dtype']:\n",
    "                errors.append(f\"Data type mismatch: expected {rules['dtype']}, got {df[column].dtype}\")\n",
    "\n",
    "        if 'min' in rules:\n",
    "            if df[column].dtype in ['int64', 'float64']:\n",
    "                if any(df[column] < rules['min']):\n",
    "                    errors.append(f\"Values below minimum: {rules['min']}\")\n",
    "            else:\n",
    "                errors.append(\"Cannot check minimum for non-numeric data type\")\n",
    "\n",
    "        if 'max' in rules:\n",
    "            if df[column].dtype in ['int64', 'float64']:\n",
    "                if any(df[column] > rules['max']):\n",
    "                    errors.append(f\"Values above maximum: {rules['max']}\")\n",
    "            else:\n",
    "                errors.append(\"Cannot check maximum for non-numeric data type\")\n",
    "\n",
    "        if 'allowed_values' in rules:\n",
    "            if df[column].dtype == 'object':\n",
    "                invalid_values = df[column][~df[column].isin(rules['allowed_values'])].unique()\n",
    "                if len(invalid_values) > 0:\n",
    "                    errors.append(f\"Invalid values: {invalid_values}\")\n",
    "            else:\n",
    "                errors.append(\"Cannot check allowed values for non-object data type\")\n",
    "\n",
    "        if errors:\n",
    "            validation_errors[column] = errors\n",
    "\n",
    "    return validation_errors\n",
    "\n",
    "validation_results = validate_data(df, constraints)\n",
    "\n",
    "if validation_results:\n",
    "    print(\"\\nData Validation Errors:\")\n",
    "    for column, errors in validation_results.items():\n",
    "        print(f\"Column '{column}':\")\n",
    "        for error in errors:\n",
    "            print(f\"- {error}\")\n",
    "else:\n",
    "    print(\"\\nData validation successful. No errors found.\")\n",
    "\n",
    "# Task 3: Detect Missing Data Patterns\n",
    "# Description: Analyze and visualize missing data patterns in a dataset.\n",
    "# Steps:\n",
    "# 1. Visualize missing data using a heatmap.\n",
    "# 2. Identify patterns in missing data.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the same DataFrame 'df' from Task 1\n",
    "\n",
    "# 1. Visualize missing data using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 2. Identify patterns in missing data\n",
    "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"\\nNumber of missing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.size\n",
    "percent_missing = (total_missing / total_cells) * 100\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "print(f\"Percentage of missing data: {percent_missing:.2f}%\")\n",
    "\n",
    "# Investigate if missing values in one column are related to missing values in another\n",
    "missing_matrix = df.isnull()\n",
    "missing_pairs = {}\n",
    "for col1 in df.columns:\n",
    "    for col2 in df.columns:\n",
    "        if col1 != col2:\n",
    "            both_missing = missing_matrix[col1] & missing_matrix[col2]\n",
    "            if both_missing.sum() > 0:\n",
    "                key = tuple(sorted((col1, col2)))\n",
    "                if key not in missing_pairs:\n",
    "                    missing_pairs[key] = both_missing.sum()\n",
    "print(\"\\nColumns with co-occurring missing values:\")\n",
    "print(missing_pairs)\n",
    "\n",
    "# Task 4: Integrate Automated Data Quality Checks\n",
    "# Description: Integrate automated data quality checks using the Great Expectations library for a dataset.\n",
    "# Steps:\n",
    "# 1. Install and initialize Great Expectations.\n",
    "# 2. Set up Great Expectations.\n",
    "# 3. Add further checks and validate.\n",
    "\n",
    "# Note: This task requires the Great Expectations library to be installed.\n",
    "# You can install it using: pip install great_expectations\n",
    "\n",
    "# The following code provides a basic structure. Running it fully requires\n",
    "# setting up a Great Expectations project, which involves more steps than can\n",
    "# be included in a simple code snippet.\n",
    "\n",
    "# 1. Install and initialize Great Expectations (command-line steps usually)\n",
    "#    - Run: great_expectations init\n",
    "\n",
    "# 2. Set up Great Expectations (involves configuring data sources and expectations)\n",
    "#    - This typically involves editing configuration files created by the init command.\n",
    "#    - You would define a data source pointing to your 'your_dataset.csv' file.\n",
    "#    - You would then create an Expectation Suite, which is a collection of data quality checks.\n",
    "\n",
    "# 3. Add further checks and validate (using Python code)\n",
    "# Assuming you have initialized Great Expectations and have a DataContext\n",
    "# and an Expectation Suite set up.\n",
    "\n",
    "# from great_expectations.data_context import DataContext\n",
    "\n",
    "# # Load your DataContext (replace with your actual project directory)\n",
    "# context = DataContext(\"./great_expectations\")\n",
    "\n",
    "# # Get a validator for your data (assuming a Pandas DataFrame datasource named 'pandas_default' and a data asset named 'my_data')\n",
    "# validator = context.get_validator(\n",
    "#     datasource_name=\"pandas_default\",\n",
    "#     data_asset_name=\"your_dataset\", # You might need to configure this in GE\n",
    "# )\n",
    "\n",
    "# # Add expectations (data quality checks) to the validator\n",
    "# validator.expect_column_to_exist(\"column_name_1\")\n",
    "# validator.expect_column_values_to_be_of_type(\"column_name_1\", \"INTEGER\")\n",
    "# validator.expect_column_values_to_be_between(\"column_name_1\", min_value=0, max_value=100)\n",
    "# validator.expect_column_values_to_not_be_null(\"column_name_2\")\n",
    "# validator.expect_column_values_to_be_in_set(\"column_name_2\", [\"A\", \"B\", \"C\"])\n",
    "# # Add more expectations for other columns\n",
    "\n",
    "# # Validate the data against the expectations\n",
    "# validation_results = validator.validate()\n",
    "\n",
    "# # Print the validation results\n",
    "# print(\"\\nGreat Expectations Validation Results:\")\n",
    "# print(validation_results)\n",
    "\n",
    "# # You can then use the validation_results to determine if your data meets the defined quality standards.\n",
    "# if validation_results[\"success\"]:\n",
    "#     print(\"Data quality checks passed!\")\n",
    "# else:\n",
    "#     print(\"Data quality checks failed. See the validation results for details.\")\n",
    "\n",
    "print(\"\\nFor Task 4, please ensure you have the Great Expectations library installed and initialized.\")\n",
    "print(\"Refer to the Great Expectations documentation for detailed steps on setting up data sources, expectation suites, and running validations.\")\n",
    "print(\"A basic initialization can be done via the command line: `great_expectations init`\")\n",
    "print(\"Then, you would typically edit the `great_expectations.yml` file and use Python code to define and run expectations.\")\n",
    "\n",
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Detecting Missing Values during Data Ingestion\n",
    "**Description**: You have a CSV file with missing values in some columns. Write a Python script to detect and report missing values during the ingestion process.\n",
    "\n",
    "**Steps**:\n",
    "1. Load data\n",
    "2. Check for missing values\n",
    "3. Report missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at your_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "\n",
    "def detect_missing_values(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, detects missing values, and reports them.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully from: {file_path}\\n\")\n",
    "\n",
    "        # Step 2: Check for missing values\n",
    "        missing_values = df.isnull().sum()\n",
    "\n",
    "        # Step 3: Report missing values\n",
    "        if missing_values.any():\n",
    "            print(\"Missing values detected in the following columns:\")\n",
    "            print(missing_values[missing_values > 0])\n",
    "            total_missing = missing_values.sum()\n",
    "            total_rows = len(df)\n",
    "            percentage_missing = (total_missing / (total_rows * len(df.columns))) * 100\n",
    "            print(f\"\\nTotal number of missing values: {total_missing}\")\n",
    "            print(f\"Total number of rows: {total_rows}\")\n",
    "            print(f\"Total number of cells: {total_rows * len(df.columns)}\")\n",
    "            print(f\"Percentage of missing values in the entire dataset: {percentage_missing:.2f}%\")\n",
    "        else:\n",
    "            print(\"No missing values found in the dataset.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'your_data.csv'  # Replace 'your_data.csv' with the actual path to your CSV file\n",
    "detect_missing_values(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Validate Data Types during Extraction\n",
    "**Description**: You have a JSON file that should have specific data types for each field. Write a script to validate if the data types match the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Define expected schema\n",
    "2. Validate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from: your_data.json\n",
      "\n",
      "Data type validation failed. Errors found:\n",
      "- Record 2, Field 'age': Expected type 'int', but got 'str'\n",
      "- Record 2, Field 'is_active': Expected type 'bool', but got 'str'\n",
      "- Record 2, Field 'salary': Expected type 'float', but got 'int'\n",
      "- Record 3: Missing field 'is_active'\n",
      "\n",
      "Validation Result: False\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import json\n",
    "\n",
    "def validate_data_types(file_path, expected_schema):\n",
    "    \"\"\"\n",
    "    Loads data from a JSON file and validates if the data types of each field\n",
    "    match the expected schema.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "        expected_schema (dict): A dictionary defining the expected data types\n",
    "                                 for each field. For example:\n",
    "                                 {'name': str, 'age': int, 'is_active': bool, 'salary': float}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Data loaded successfully from: {file_path}\\n\")\n",
    "\n",
    "        if not isinstance(data, list):\n",
    "            data = [data]  # Handle single JSON object as a list of one\n",
    "\n",
    "        validation_errors = []\n",
    "\n",
    "        for index, record in enumerate(data):\n",
    "            for field, expected_type in expected_schema.items():\n",
    "                if field not in record:\n",
    "                    validation_errors.append(f\"Record {index + 1}: Missing field '{field}'\")\n",
    "                    continue\n",
    "\n",
    "                actual_value = record[field]\n",
    "                actual_type = type(actual_value)\n",
    "\n",
    "                if actual_type != expected_type:\n",
    "                    validation_errors.append(\n",
    "                        f\"Record {index + 1}, Field '{field}': Expected type '{expected_type.__name__}', but got '{actual_type.__name__}'\"\n",
    "                    )\n",
    "\n",
    "        if validation_errors:\n",
    "            print(\"Data type validation failed. Errors found:\")\n",
    "            for error in validation_errors:\n",
    "                print(f\"- {error}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Data type validation successful. All fields match the expected schema.\")\n",
    "            return True\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return False\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example Usage:\n",
    "file_path = 'your_data.json'  # Replace 'your_data.json' with the actual path to your JSON file\n",
    "\n",
    "# Define the expected schema\n",
    "expected_schema = {\n",
    "    'name': str,\n",
    "    'age': int,\n",
    "    'is_active': bool,\n",
    "    'salary': float,\n",
    "    'city': str  # Example of another field\n",
    "}\n",
    "\n",
    "# Create a sample JSON file for testing (optional)\n",
    "sample_data = [\n",
    "    {'name': 'Alice', 'age': 30, 'is_active': True, 'salary': 60000.50, 'city': 'Bengaluru'},\n",
    "    {'name': 'Bob', 'age': '25', 'is_active': 'true', 'salary': 55000, 'city': 'Mumbai'},\n",
    "    {'name': 'Charlie', 'age': 40, 'salary': 70000.00, 'city': 'Delhi'}\n",
    "]\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(sample_data, f, indent=4)\n",
    "\n",
    "# Validate the data types\n",
    "validation_result = validate_data_types(file_path, expected_schema)\n",
    "print(f\"\\nValidation Result: {validation_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Remove Duplicate Records in Data\n",
    "**Description**: You have a dataset with duplicate entries. Write a Python script to find and remove duplicate records using Pandas.\n",
    "\n",
    "**Steps**:\n",
    "1. Find duplicate records\n",
    "2. Remove duplicates\n",
    "3. Report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from: your_data_with_duplicates.csv\n",
      "\n",
      "Number of duplicate rows found: 4\n",
      "\n",
      "Example of duplicate rows:\n",
      "  col1  col2   col3\n",
      "0    A     1   True\n",
      "1    B     2  False\n",
      "3    A     1   True\n",
      "4    B     2  False\n",
      "\n",
      "Note: 'keep=False' shows all rows that are duplicates.\n",
      "\n",
      "Number of rows after removing duplicates: 4\n",
      "Number of duplicate rows removed: 2\n",
      "\n",
      "Cleaned DataFrame (considering all columns):\n",
      "  col1  col2   col3\n",
      "0    A     1   True\n",
      "1    B     2  False\n",
      "2    C     3   True\n",
      "5    D     4  False\n",
      "Data loaded successfully from: your_data_with_duplicates.csv\n",
      "\n",
      "Number of duplicate rows found: 4\n",
      "\n",
      "Example of duplicate rows:\n",
      "  col1  col2   col3\n",
      "0    A     1   True\n",
      "1    B     2  False\n",
      "3    A     1   True\n",
      "4    B     2  False\n",
      "\n",
      "Note: 'keep=False' shows all rows that are duplicates.\n",
      "\n",
      "Number of rows after removing duplicates: 4\n",
      "Number of duplicate rows removed: 2\n",
      "\n",
      "Cleaned DataFrame (considering columns: ['col1', 'col2']):\n",
      "  col1  col2   col3\n",
      "0    A     1   True\n",
      "1    B     2  False\n",
      "2    C     3   True\n",
      "5    D     4  False\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "\n",
    "def remove_duplicate_records(file_path, columns_to_consider=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, finds and removes duplicate records using Pandas,\n",
    "    and reports the results.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        columns_to_consider (list, optional): A list of column names to consider\n",
    "                                              when identifying duplicates. If None,\n",
    "                                              all columns are used. Defaults to None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully from: {file_path}\\n\")\n",
    "\n",
    "        # Step 2: Find duplicate records\n",
    "        initial_row_count = len(df)\n",
    "        if columns_to_consider:\n",
    "            duplicate_rows = df[df.duplicated(subset=columns_to_consider, keep=False)]\n",
    "        else:\n",
    "            duplicate_rows = df[df.duplicated(keep=False)]\n",
    "\n",
    "        num_duplicates = len(duplicate_rows)\n",
    "        print(f\"Number of duplicate rows found: {num_duplicates}\\n\")\n",
    "        if not duplicate_rows.empty:\n",
    "            print(\"Example of duplicate rows:\")\n",
    "            print(duplicate_rows.head())\n",
    "            print(\"\\nNote: 'keep=False' shows all rows that are duplicates.\")\n",
    "\n",
    "        # Step 3: Remove duplicates\n",
    "        if columns_to_consider:\n",
    "            df_cleaned = df.drop_duplicates(subset=columns_to_consider, keep='first')\n",
    "        else:\n",
    "            df_cleaned = df.drop_duplicates(keep='first')\n",
    "\n",
    "        final_row_count = len(df_cleaned)\n",
    "        removed_count = initial_row_count - final_row_count\n",
    "        print(f\"\\nNumber of rows after removing duplicates: {final_row_count}\")\n",
    "        print(f\"Number of duplicate rows removed: {removed_count}\")\n",
    "\n",
    "        return df_cleaned\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example Usage:\n",
    "file_path = 'your_data_with_duplicates.csv'  # Replace with the actual path\n",
    "\n",
    "# Create a sample CSV file with duplicates for testing (optional)\n",
    "data = {'col1': ['A', 'B', 'C', 'A', 'B', 'D'],\n",
    "        'col2': [1, 2, 3, 1, 2, 4],\n",
    "        'col3': [True, False, True, True, False, False]}\n",
    "df_sample = pd.DataFrame(data)\n",
    "df_sample.to_csv(file_path, index=False)\n",
    "\n",
    "# Remove duplicates considering all columns\n",
    "cleaned_df_all_cols = remove_duplicate_records(file_path)\n",
    "if cleaned_df_all_cols is not None:\n",
    "    print(\"\\nCleaned DataFrame (considering all columns):\")\n",
    "    print(cleaned_df_all_cols)\n",
    "\n",
    "# Remove duplicates considering only specific columns\n",
    "columns_to_check = ['col1', 'col2']\n",
    "cleaned_df_subset_cols = remove_duplicate_records(file_path, columns_to_check)\n",
    "if cleaned_df_subset_cols is not None:\n",
    "    print(f\"\\nCleaned DataFrame (considering columns: {columns_to_check}):\")\n",
    "    print(cleaned_df_subset_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Task 1: Impute with Mean or Median ---\n",
      "Missing columns in Boston Housing dataset: ['RM', 'LSTAT']\n",
      "\n",
      "Boston Housing dataset with mean and median imputation:\n",
      "      RM  RM_mean_imputed  LSTAT  LSTAT_median_imputed\n",
      "0  6.575            6.575   4.98                  4.98\n",
      "1  6.421            6.421   9.14                  9.14\n",
      "2  7.185            7.185   4.03                  4.03\n",
      "3  6.998            6.998   2.94                  2.94\n",
      "4  7.147            7.147   5.33                  5.33\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Task 2: Impute with the Most Frequent Value ---\n",
      "Missing categorical columns in Titanic dataset: ['embarked', 'embark_town']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m missing_categorical_cols_titanic:\n\u001b[1;32m     51\u001b[0m     imputer_mode \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mtitanic_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_mode_imputed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m imputer_mode\u001b[38;5;241m.\u001b[39mfit_transform(titanic_df[[col]])\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTitanic dataset with mode imputation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(titanic_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membarked\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membarked_mode_imputed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeck\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeck_mode_imputed\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4300\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4292\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4293\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4300\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4303\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4306\u001b[0m     ):\n\u001b[1;32m   4307\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/frame.py:5040\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   5039\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m-> 5040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/construction.py:608\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    606\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 608\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    610\u001b[0m         object_index\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    613\u001b[0m     ):\n\u001b[1;32m    614\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1172\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(value))  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;66;03m# Caller is responsible\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(value\u001b[38;5;241m.\u001b[39mndim)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectKBest\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Task 1: Impute with Mean or Median ---\n",
    "print(\"--- Task 1: Impute with Mean or Median ---\")\n",
    "# Load the Boston Housing dataset (note: it's deprecated, using alternative loading)\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "boston_data = pd.DataFrame(np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]))\n",
    "boston_feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "boston_target = pd.DataFrame(raw_df.values[1::2, 2], columns=['MEDV'])\n",
    "boston_df = pd.concat([boston_data, boston_target], axis=1)\n",
    "boston_df.columns = boston_feature_names + ['MEDV']\n",
    "\n",
    "# Introduce missing values for demonstration\n",
    "boston_df.loc[[10, 50, 100], 'RM'] = np.nan\n",
    "boston_df.loc[[25, 75, 150], 'LSTAT'] = np.nan\n",
    "\n",
    "# Identify columns with missing values\n",
    "missing_cols_boston = boston_df.columns[boston_df.isnull().any()].tolist()\n",
    "print(\"Missing columns in Boston Housing dataset:\", missing_cols_boston)\n",
    "\n",
    "# Impute missing values using mean and median\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "boston_df['RM_mean_imputed'] = imputer_mean.fit_transform(boston_df[['RM']])\n",
    "\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "boston_df['LSTAT_median_imputed'] = imputer_median.fit_transform(boston_df[['LSTAT']])\n",
    "\n",
    "print(\"\\nBoston Housing dataset with mean and median imputation:\")\n",
    "print(boston_df[['RM', 'RM_mean_imputed', 'LSTAT', 'LSTAT_median_imputed']].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 2: Impute with the Most Frequent Value ---\n",
    "print(\"--- Task 2: Impute with the Most Frequent Value ---\")\n",
    "# Load the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Identify categorical columns with missing values\n",
    "categorical_cols_titanic = titanic_df.select_dtypes(include=['object']).columns\n",
    "missing_categorical_cols_titanic = [col for col in categorical_cols_titanic if titanic_df[col].isnull().any()]\n",
    "print(\"Missing categorical columns in Titanic dataset:\", missing_categorical_cols_titanic)\n",
    "\n",
    "# Impute missing categorical values using the most frequent value (mode)\n",
    "for col in missing_categorical_cols_titanic:\n",
    "    imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "    titanic_df[col + '_mode_imputed'] = imputer_mode.fit_transform(titanic_df[[col]])\n",
    "\n",
    "print(\"\\nTitanic dataset with mode imputation:\")\n",
    "print(titanic_df[['embarked', 'embarked_mode_imputed', 'deck', 'deck_mode_imputed']].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 3: Advanced Imputation - k-Nearest Neighbors ---\n",
    "print(\"--- Task 3: Advanced Imputation - k-Nearest Neighbors ---\")\n",
    "# Create a subset of numerical features from Boston Housing for KNN imputation\n",
    "boston_numerical = boston_df[['RM', 'LSTAT', 'PTRATIO']].copy()\n",
    "boston_numerical.loc[[15, 65, 120], 'PTRATIO'] = np.nan\n",
    "\n",
    "print(\"\\nSubset of Boston Housing data with missing values for KNN Imputation:\")\n",
    "print(boston_numerical.head())\n",
    "\n",
    "# Implement KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "boston_numerical_imputed = pd.DataFrame(knn_imputer.fit_transform(boston_numerical), columns=boston_numerical.columns)\n",
    "\n",
    "print(\"\\nSubset of Boston Housing data after KNN Imputation:\")\n",
    "print(boston_numerical_imputed.head())\n",
    "\n",
    "print(\"\\nKNN imputation can often provide more accurate imputations compared to simple methods like mean or mode, especially when the missing values have dependencies on other features. It leverages the information from similar data points to estimate the missing values.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 1: Standardization ---\n",
    "print(\"--- Feature Scaling & Normalization Best Practices: ---\")\n",
    "print(\"--- Task 1: Standardization ---\")\n",
    "# Create a sample numerical Series\n",
    "numerical_data_standardization = pd.Series([10, 20, 15, 25, 30, 12, 18, 22])\n",
    "print(\"Original numerical data:\")\n",
    "print(numerical_data_standardization)\n",
    "\n",
    "# Standardize the data using StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "standardized_data = scaler_standard.fit_transform(numerical_data_standardization.values.reshape(-1, 1))\n",
    "standardized_series = pd.Series(standardized_data.flatten())\n",
    "\n",
    "print(\"\\nStandardized data:\")\n",
    "print(standardized_series)\n",
    "\n",
    "print(\"\\nStandardization scales the data to have a mean of 0 and a standard deviation of 1. This can be beneficial for algorithms that are sensitive to the scale of the input features, such as gradient-based methods. It centers the data around zero.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 2: Min-Max Scaling ---\n",
    "print(\"--- Task 2: Min-Max Scaling ---\")\n",
    "# Scale the original numerical data using MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "minmax_scaled_data = scaler_minmax.fit_transform(numerical_data_standardization.values.reshape(-1, 1))\n",
    "minmax_scaled_series = pd.Series(minmax_scaled_data.flatten())\n",
    "\n",
    "print(\"\\nMin-Max scaled data (range [0, 1]):\")\n",
    "print(minmax_scaled_series)\n",
    "\n",
    "print(\"\\nMin-Max scaling scales the data to a fixed range, typically between 0 and 1. This can be useful when you need values within a specific range, for example, for some neural network activation functions. Compared to standardization, it preserves the original shape of the data distribution.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 3: Robust Scaling ---\n",
    "print(\"--- Task 3: Robust Scaling ---\")\n",
    "# Introduce outliers to the sample data\n",
    "numerical_data_outliers = pd.Series([10, 20, 15, 25, 30, 12, 18, 22, 100, -5])\n",
    "print(\"Numerical data with outliers:\")\n",
    "print(numerical_data_outliers)\n",
    "\n",
    "# Scale the data using RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaled_data = robust_scaler.fit_transform(numerical_data_outliers.values.reshape(-1, 1))\n",
    "robust_scaled_series = pd.Series(robust_scaled_data.flatten())\n",
    "\n",
    "print(\"\\nRobust scaled data:\")\n",
    "print(robust_scaled_series)\n",
    "\n",
    "print(\"\\nRobust scaling uses the median and interquartile range (IQR) to scale the data. It is less affected by outliers compared to Min-Max scaling and standardization because it does not rely on the mean and standard deviation, which can be significantly influenced by extreme values. This makes it a good choice when your data contains outliers.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Feature Selection Techniques: ---\n",
    "print(\"--- Feature Selection Techniques: ---\")\n",
    "# --- Task 1: Correlation Matrix ---\n",
    "print(\"--- Task 1: Correlation Matrix ---\")\n",
    "# Compute the correlation matrix for the Boston Housing dataset\n",
    "correlation_matrix = boston_df.corr()\n",
    "print(\"\\nCorrelation Matrix of Boston Housing Dataset:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Identify highly correlated features (correlation > 0.9)\n",
    "upper_triangle = np.triu(correlation_matrix, k=1)\n",
    "highly_correlated_pairs = np.where(np.abs(upper_triangle) > 0.9)\n",
    "highly_correlated_features = [(correlation_matrix.columns[i], correlation_matrix.columns[j])\n",
    "                             for i, j in zip(*highly_correlated_pairs)]\n",
    "\n",
    "print(\"\\nHighly correlated features (correlation > 0.9):\", highly_correlated_features)\n",
    "\n",
    "# To remove one of the highly correlated features, you would typically choose based on domain knowledge or which feature is less important. For example, let's arbitrarily decide to remove the second feature in each pair.\n",
    "features_to_drop = [pair[1] for pair in highly_correlated_features]\n",
    "boston_df_reduced_corr = boston_df.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "print(\"\\nBoston Housing DataFrame after removing highly correlated features:\", boston_df_reduced_corr.shape)\n",
    "\n",
    "print(\"\\nRemoving highly correlated features can help to reduce multicollinearity in your data, which can improve the performance and interpretability of some models, especially linear models.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Using Mutual Information & Variance Thresholds: ---\n",
    "print(\"--- Using Mutual Information & Variance Thresholds: ---\")\n",
    "# --- Task 2: Mutual Information ---\n",
    "print(\"--- Task 2: Mutual Information ---\")\n",
    "# Load a classification dataset (using a simple one for demonstration)\n",
    "data_mi = {'feature_1': [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],\n",
    "           'feature_2': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1],\n",
    "           'feature_3': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B', 'C', 'A'],\n",
    "           'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]}\n",
    "df_mi = pd.DataFrame(data_mi)\n",
    "df_mi = pd.get_dummies(df_mi, columns=['feature_3'], drop_first=True) # One-hot encode categorical features\n",
    "\n",
    "X_mi = df_mi.drop('target', axis=1)\n",
    "y_mi = df_mi['target']\n",
    "\n",
    "# Compute mutual information between features and target\n",
    "mutual_info = mutual_info_classif(X_mi, y_mi)\n",
    "mutual_info_series = pd.Series(mutual_info, index=X_mi.columns)\n",
    "mutual_info_sorted = mutual_info_series.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nMutual Information between features and target:\")\n",
    "print(mutual_info_sorted)\n",
    "\n",
    "# Retain features with high mutual information scores (e.g., top 2)\n",
    "k_best = 2\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=k_best)\n",
    "X_mi_selected = selector_mi.fit_transform(X_mi, y_mi)\n",
    "selected_features_mi = X_mi.columns[selector_mi.get_support(indices=True)]\n",
    "\n",
    "print(f\"\\nTop {k_best} features with highest mutual information:\", selected_features_mi)\n",
    "\n",
    "print(\"\\nMutual information measures the statistical dependency between two random variables. In feature selection, it helps to identify features that have a strong relationship with the target variable, which can be useful for prediction.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Task 3: Variance Threshold ---\n",
    "print(\"--- Task 3: Variance Threshold ---\")\n",
    "# Create a DataFrame with some low variance features\n",
    "data_variance = {'feature_1': [1, 1, 1, 1, 1],\n",
    "                 'feature_2': [2, 2, 3, 2, 2],\n",
    "                 'feature_3': [10, 20, 15, 25, 30],\n",
    "                 'feature_4': [0, 0, 0, 0, 0]}\n",
    "df_variance = pd.DataFrame(data_variance)\n",
    "\n",
    "print(\"\\nOriginal DataFrame for Variance Threshold:\")\n",
    "print(df_variance)\n",
    "\n",
    "# Implement VarianceThreshold to remove features with low variance\n",
    "threshold_vt = 0.1\n",
    "selector_vt = VarianceThreshold(threshold=threshold_vt)\n",
    "X_variance_transformed = selector_vt.fit_transform(df_variance)\n",
    "\n",
    "# Get the names of the features that were kept\n",
    "kept_features_vt_indices = selector_vt.get_support(indices=True)\n",
    "kept_features_vt = df_variance.columns[kept_features_vt_indices]\n",
    "\n",
    "print(f\"\\nDataFrame after applying Variance Threshold (threshold={threshold_vt}):\")\n",
    "print(pd.DataFrame(X_variance_transformed, columns=kept_features_vt))\n",
    "\n",
    "print(\"\\nVariance Threshold removes features whose variance does not exceed a certain threshold. Features with very low variance contain little information and might not be helpful for modeling. Analyzing the impact on the feature space involves seeing which features are removed and understanding if this aligns with your expectations about the importance of those features.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

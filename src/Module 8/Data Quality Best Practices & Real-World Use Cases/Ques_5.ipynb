{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI/ML â€“ Improving Model Performance with Clean Data\n",
    "\n",
    "**Task 1**: Data Preprocessing for Models\n",
    "\n",
    "**Objective**: Enhance data quality for better AI/ML outcomes.\n",
    "\n",
    "**Steps**:\n",
    "1. Choose a dataset for training an AI/ML model.\n",
    "2. Identify common data issues like null values, redundant features, or noisydata.\n",
    "3. Apply preprocessing methods such as imputation, normalization, or feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Iris Dataset:\n",
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Original Iris Dataset:\")\n",
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Introduce null values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m iris_with_na \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m na_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(iris\u001b[38;5;241m.\u001b[39mindex, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m na_indices:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Introduce null values\n",
    "iris_with_na = iris.copy()\n",
    "np.random.seed(42)\n",
    "na_indices = np.random.choice(iris.index, size=15, replace=False)\n",
    "for i in na_indices:\n",
    "    random_col = np.random.choice(iris.columns[:-1]) # Avoid introducing NA in the target variable\n",
    "    iris_with_na.loc[i, random_col] = None\n",
    "\n",
    "# Introduce outliers\n",
    "iris_with_outliers = iris_with_na.copy()\n",
    "iris_with_outliers.loc[20, 'sepal_length'] = 15.0\n",
    "iris_with_outliers.loc[35, 'petal_width'] = 0.1\n",
    "\n",
    "# Introduce a redundant feature (highly correlated with sepal_length)\n",
    "iris_with_redundant = iris_with_outliers.copy()\n",
    "iris_with_redundant['sepal_length_plus_noise'] = iris_with_redundant['sepal_length'] + np.random.normal(0, 0.1, len(iris))\n",
    "\n",
    "# Display the dataset with introduced issues\n",
    "print(\"\\nIris Dataset with Introduced Issues (First few rows):\")\n",
    "print(iris_with_redundant.head())\n",
    "\n",
    "# Check for null values\n",
    "print(\"\\nNumber of Null Values per Column:\")\n",
    "print(iris_with_redundant.isnull().sum())\n",
    "\n",
    "# Visualize potential outliers (simple box plots)\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(data=iris_with_redundant.drop(columns=['species']))\n",
    "plt.title(\"Box Plots of Features (with potential outliers)\")\n",
    "plt.show()\n",
    "\n",
    "# Check correlation matrix to see potential redundancy\n",
    "correlation_matrix = iris_with_redundant.drop(columns=['species']).corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Handle null values using imputation (mean strategy)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "iris_imputed = pd.DataFrame(imputer.fit_transform(iris_with_redundant.drop(columns=['species'])), columns=iris.columns[:-1])\n",
    "iris_imputed['species'] = iris_with_redundant['species']\n",
    "\n",
    "print(\"\\nIris Dataset After Imputation (First few rows):\")\n",
    "print(iris_imputed.head())\n",
    "print(\"\\nNumber of Null Values After Imputation:\")\n",
    "print(iris_imputed.isnull().sum())\n",
    "\n",
    "# Handle outliers (simple capping for demonstration - more sophisticated methods exist)\n",
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "iris_no_outliers = iris_imputed.copy()\n",
    "for col in iris_no_outliers.columns[:-1]:\n",
    "    iris_no_outliers[col] = cap_outliers(iris_no_outliers[col])\n",
    "\n",
    "# Visualize after outlier capping\n",
    "sns.boxplot(data=iris_no_outliers.drop(columns=['species']))\n",
    "plt.title(\"Box Plots After Outlier Capping\")\n",
    "plt.show()\n",
    "\n",
    "# Handle redundant features (simple removal - more advanced techniques like PCA exist)\n",
    "iris_no_redundant = iris_no_outliers.drop(columns=['sepal_length_plus_noise'])\n",
    "print(\"\\nIris Dataset After Redundant Feature Removal (First few rows):\")\n",
    "print(iris_no_redundant.head())\n",
    "\n",
    "# Feature Scaling (Normalization/Standardization)\n",
    "scaler = StandardScaler()\n",
    "X = iris_no_redundant.drop(columns=['species'])\n",
    "y = iris_no_redundant['species']\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "iris_scaled = pd.concat([X_scaled, y], axis=1)\n",
    "\n",
    "print(\"\\nScaled Iris Dataset (First few rows):\")\n",
    "print(iris_scaled.head())\n",
    "\n",
    "# For inconsistent categorical data (not prominent here, but an example):\n",
    "# iris_processed['species'] = iris_processed['species'].str.lower().str.strip()\n",
    "\n",
    "print(\"\\nProcessed Iris Dataset (Ready for Model Training - First few rows):\")\n",
    "print(iris_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Evaluate Model Performance\n",
    "\n",
    "**Objective**: Assess the impact of data quality improvements on model performance.\n",
    "\n",
    "**Steps**:\n",
    "1. Train a simple ML model with and without preprocessing.\n",
    "2. Analyze and compare model performance metrics to evaluate the impact of data quality strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Introduce the same data quality issues as before\u001b[39;00m\n\u001b[1;32m     12\u001b[0m iris_with_issues \u001b[38;5;241m=\u001b[39m iris_original\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     14\u001b[0m na_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(iris_original\u001b[38;5;241m.\u001b[39mindex, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m na_indices:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset again to have a clean starting point for the 'original'\n",
    "iris_original = sns.load_dataset('iris')\n",
    "\n",
    "# Introduce the same data quality issues as before\n",
    "iris_with_issues = iris_original.copy()\n",
    "np.random.seed(42)\n",
    "na_indices = np.random.choice(iris_original.index, size=15, replace=False)\n",
    "for i in na_indices:\n",
    "    random_col = np.random.choice(iris_original.columns[:-1])\n",
    "    iris_with_issues.loc[i, random_col] = None\n",
    "iris_with_outliers = iris_with_issues.copy()\n",
    "iris_with_outliers.loc[20, 'sepal_length'] = 15.0\n",
    "iris_with_outliers.loc[35, 'petal_width'] = 0.1\n",
    "iris_with_redundant = iris_with_outliers.copy()\n",
    "iris_with_redundant['sepal_length_plus_noise'] = iris_with_redundant['sepal_length'] + np.random.normal(0, 0.1, len(iris_original))\n",
    "\n",
    "# Preprocess the dataset (as done in the previous task)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "iris_imputed = pd.DataFrame(imputer.fit_transform(iris_with_redundant.drop(columns=['species'])), columns=iris_original.columns[:-1])\n",
    "iris_imputed['species'] = iris_with_redundant['species']\n",
    "\n",
    "# Cap outliers\n",
    "def cap_outliers(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower_bound = series.quantile(lower_quantile)\n",
    "    upper_bound = series.quantile(upper_quantile)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "iris_no_outliers = iris_imputed.copy()\n",
    "for col in iris_no_outliers.columns[:-1]:\n",
    "    iris_no_outliers[col] = cap_outliers(iris_no_outliers[col])\n",
    "\n",
    "# Remove redundant feature\n",
    "iris_preprocessed = iris_no_outliers.drop(columns=['sepal_length_plus_noise'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_processed = iris_preprocessed.drop(columns=['species'])\n",
    "y_processed = iris_preprocessed['species']\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_processed), columns=X_processed.columns)\n",
    "iris_scaled = pd.concat([X_scaled, y_processed], axis=1)\n",
    "\n",
    "# 1. Train a simple ML model with and without preprocessing\n",
    "\n",
    "# Model 1: Trained on data with issues\n",
    "X_original = iris_with_redundant.drop(columns=['species'])\n",
    "y_original = iris_with_redundant['species']\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y_original, test_size=0.3, random_state=42)\n",
    "\n",
    "model_original = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "model_original.fit(X_train_orig, y_train_orig)\n",
    "y_pred_orig = model_original.predict(X_test_orig)\n",
    "accuracy_original = accuracy_score(y_test_orig, y_pred_orig)\n",
    "\n",
    "print(\"\\n--- Model Performance on Data with Issues ---\")\n",
    "print(f\"Accuracy: {accuracy_original:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_orig, y_pred_orig))\n",
    "\n",
    "# Model 2: Trained on preprocessed data\n",
    "X_processed_scaled = iris_scaled.drop(columns=['species'])\n",
    "y_processed_scaled = iris_scaled['species']\n",
    "X_train_processed, X_test_processed, y_train_processed, y_test_processed = train_test_split(X_processed_scaled, y_processed_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "model_processed = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "model_processed.fit(X_train_processed, y_train_processed)\n",
    "y_pred_processed = model_processed.predict(X_test_processed)\n",
    "accuracy_processed = accuracy_score(y_test_processed, y_pred_processed)\n",
    "\n",
    "print(\"\\n--- Model Performance on Preprocessed Data ---\")\n",
    "print(f\"Accuracy: {accuracy_processed:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_processed, y_pred_processed))\n",
    "\n",
    "# 2. Analyze and compare model performance metrics\n",
    "\n",
    "print(\"\\n--- Comparison of Model Performance ---\")\n",
    "print(f\"Accuracy on Data with Issues:     {accuracy_original:.4f}\")\n",
    "print(f\"Accuracy on Preprocessed Data: {accuracy_processed:.4f}\")\n",
    "\n",
    "if accuracy_processed > accuracy_original:\n",
    "    improvement = (accuracy_processed - accuracy_original) / accuracy_original * 100\n",
    "    print(f\"\\nAccuracy improved by {improvement:.2f}% after preprocessing.\")\n",
    "elif accuracy_processed < accuracy_original:\n",
    "    decline = (accuracy_original - accuracy_processed) / accuracy_original * 100\n",
    "    print(f\"\\nAccuracy declined by {decline:.2f}% after preprocessing (this could happen due to aggressive preprocessing or the nature of the issues).\")\n",
    "else:\n",
    "    print(\"\\nAccuracy remained the same after preprocessing.\")\n",
    "\n",
    "print(\"\\nObservations from Classification Reports:\")\n",
    "print(\"- Look at precision, recall, and F1-score for each class in both reports.\")\n",
    "print(\"- See if preprocessing helped in better classifying specific species (e.g., higher recall for a previously poorly classified species).\")\n",
    "print(\"- Note any changes in the support (number of samples) for each class in the test sets (should be similar due to the same split).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values - Imputation within ML Pipelines\n",
    "**Description**: Implement a machine learning pipeline that includes imputation and a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "This 'Pipeline' has no attribute 'fit_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Preprocess the training data (handling categorical features separately for now)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m X_train_numerical \u001b[38;5;241m=\u001b[39m numerical_pipeline\u001b[38;5;241m.\u001b[39mfit_transform(X_train[numerical_features_pipeline])\n\u001b[0;32m---> 54\u001b[0m X_train_categorical \u001b[38;5;241m=\u001b[39m \u001b[43mcategorical_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m(X_train[categorical_features_pipeline])\n\u001b[1;32m     55\u001b[0m X_train_processed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_numerical, columns\u001b[38;5;241m=\u001b[39mnumerical_features_pipeline),\n\u001b[1;32m     56\u001b[0m                                pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_categorical, columns\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mget_dummies(X_train[categorical_features_pipeline])\u001b[38;5;241m.\u001b[39mcolumns)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py:43\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, owner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mowner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m         out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# This makes it possible to use the decorated method as an unbound method,\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# for instance when monkeypatching.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/AI_DATA_ANALYSIS_/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py:37\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor._check\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr_err_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_result:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr_err_msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: This 'Pipeline' has no attribute 'fit_transform'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a sample DataFrame with missing values and a target variable\n",
    "data_pipeline = {'numerical_feature_1': [1, 2, None, 4, 5, None, 7, 8],\n",
    "                 'numerical_feature_2': [2.1, 3.5, 1.8, 4.2, None, 2.9, 5.1, 3.8],\n",
    "                 'categorical_feature': ['A', None, 'B', 'A', 'C', 'B', None, 'A'],\n",
    "                 'target': [0, 1, 0, 1, 0, 1, 0, 1]}\n",
    "df_pipeline = pd.DataFrame(data_pipeline)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_pipeline.drop('target', axis=1)\n",
    "y = df_pipeline['target']\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features_pipeline = X.select_dtypes(include=['number']).columns\n",
    "categorical_features_pipeline = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', pd.get_dummies) # Using pandas get_dummies within the pipeline\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer (not strictly needed for this simple case)\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', numerical_pipeline, numerical_features_pipeline),\n",
    "#     ('cat', categorical_pipeline, categorical_features_pipeline)\n",
    "# ])\n",
    "\n",
    "# Create the full machine learning pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor_num', numerical_pipeline),\n",
    "    ('preprocessor_cat', categorical_pipeline),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the training data (handling categorical features separately for now)\n",
    "X_train_numerical = numerical_pipeline.fit_transform(X_train[numerical_features_pipeline])\n",
    "X_train_categorical = categorical_pipeline.fit_transform(X_train[categorical_features_pipeline])\n",
    "X_train_processed = pd.concat([pd.DataFrame(X_train_numerical, columns=numerical_features_pipeline),\n",
    "                               pd.DataFrame(X_train_categorical, columns=pd.get_dummies(X_train[categorical_features_pipeline]).columns)], axis=1)\n",
    "\n",
    "# Train the classifier\n",
    "model_pipeline.fit(X_train_processed, y_train)\n",
    "\n",
    "# Preprocess the testing data\n",
    "X_test_numerical = numerical_pipeline.transform(X_test[numerical_features_pipeline])\n",
    "X_test_categorical = categorical_pipeline.transform(X_test[categorical_features_pipeline])\n",
    "X_test_processed = pd.concat([pd.DataFrame(X_test_numerical, columns=numerical_features_pipeline),\n",
    "                              pd.DataFrame(X_test_categorical, columns=pd.get_dummies(X_test[categorical_features_pipeline]).columns)], axis=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_pipeline.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the pipeline: {accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nPipeline Steps:\")\n",
    "for name, step in model_pipeline.steps:\n",
    "    print(f\"- {name}: {step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

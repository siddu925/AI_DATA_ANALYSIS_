{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance of Data Cleaning\n",
    "\n",
    "# 1. Missing Values: Missing data points in a dataset can lead to biased results.\n",
    "#     Task 1: Load a dataset and identify which columns have missing values.\n",
    "#     Task 2: Replace missing values in a dataset with the column mean or mode.\n",
    "#     Task 3: Compare model performance with and without handling missing values.\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 1: Load a dataset and identify which columns have missing values.\n",
      "\n",
      "Original DataFrame:\n",
      "   Feature_A  Feature_B  Feature_C  Target\n",
      "0       10.0        5.0        1.0      20\n",
      "1       12.0        NaN        2.0      25\n",
      "2        NaN        7.0        3.0      30\n",
      "3       15.0        9.0        NaN      35\n",
      "4       18.0       11.0        5.0      40\n",
      "5        NaN       13.0        6.0      45\n",
      "6       22.0        NaN        7.0      50\n",
      "7       25.0       17.0        NaN      55\n",
      "\n",
      "Number of missing values per column:\n",
      "Feature_A    2\n",
      "Feature_B    2\n",
      "Feature_C    2\n",
      "Target       0\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values:\n",
      "['Feature_A', 'Feature_B', 'Feature_C']\n",
      "\n",
      "\n",
      "Task 2: Replace missing values in a dataset with the column mean or mode.\n",
      "\n",
      "DataFrame with missing numerical values imputed with the mean:\n",
      "   Feature_A  Feature_B  Feature_C  Target\n",
      "0       10.0   5.000000        1.0      20\n",
      "1       12.0  10.333333        2.0      25\n",
      "2       17.0   7.000000        3.0      30\n",
      "3       15.0   9.000000        4.0      35\n",
      "4       18.0  11.000000        5.0      40\n",
      "5       17.0  13.000000        6.0      45\n",
      "6       22.0  10.333333        7.0      50\n",
      "7       25.0  17.000000        4.0      55\n",
      "\n",
      "DataFrame with missing values imputed with the mode:\n",
      "   Feature_A  Feature_B  Feature_C  Target\n",
      "0       10.0        5.0        1.0      20\n",
      "1       12.0        5.0        2.0      25\n",
      "2       10.0        7.0        3.0      30\n",
      "3       15.0        9.0        1.0      35\n",
      "4       18.0       11.0        5.0      40\n",
      "5       10.0       13.0        6.0      45\n",
      "6       22.0        5.0        7.0      50\n",
      "7       25.0       17.0        1.0      55\n",
      "\n",
      "\n",
      "Task 3: Compare model performance with and without handling missing values.\n",
      "\n",
      "Model performance (missing values dropped): Mean Squared Error = 400.00\n",
      "Model performance (missing values imputed with mean): Mean Squared Error = 8.54\n",
      "Model performance (missing values imputed with mode): Mean Squared Error = 147.97\n",
      "Model performance (missing values imputed with median): Mean Squared Error = 6.37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Missing Values: Missing data points in a dataset can lead to biased results.\n",
    "\n",
    "# Task 1: Load a dataset and identify which columns have missing values.\n",
    "print(\"\\nTask 1: Load a dataset and identify which columns have missing values.\")\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {'Feature_A': [10, 12, np.nan, 15, 18, np.nan, 22, 25],\n",
    "        'Feature_B': [5, np.nan, 7, 9, 11, 13, np.nan, 17],\n",
    "        'Feature_C': [1, 2, 3, np.nan, 5, 6, 7, np.nan],\n",
    "        'Target': [20, 25, 30, 35, 40, 45, 50, 55]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Identify columns with missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(\"\\nNumber of missing values per column:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "columns_with_missing = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "print(\"\\nColumns with missing values:\")\n",
    "print(columns_with_missing)\n",
    "\n",
    "# Task 2: Replace missing values in a dataset with the column mean or mode.\n",
    "print(\"\\n\\nTask 2: Replace missing values in a dataset with the column mean or mode.\")\n",
    "\n",
    "df_imputed_mean = df.copy()\n",
    "df_imputed_mode = df.copy()\n",
    "\n",
    "# Impute missing numerical values with the mean\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "for col in numerical_cols:\n",
    "    df_imputed_mean[col].fillna(df_imputed_mean[col].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nDataFrame with missing numerical values imputed with the mean:\")\n",
    "print(df_imputed_mean)\n",
    "\n",
    "# Impute missing values with the mode (for all columns in this simple example)\n",
    "# Be cautious: mode is more appropriate for categorical data, but we'll demonstrate\n",
    "for col in df_imputed_mode.columns:\n",
    "    if df_imputed_mode[col].isnull().any():\n",
    "        # Calculate mode (handles multiple modes by taking the first)\n",
    "        mode_value = df_imputed_mode[col].mode()[0]\n",
    "        df_imputed_mode[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "print(\"\\nDataFrame with missing values imputed with the mode:\")\n",
    "print(df_imputed_mode)\n",
    "\n",
    "# Task 3: Compare model performance with and without handling missing values.\n",
    "print(\"\\n\\nTask 3: Compare model performance with and without handling missing values.\")\n",
    "\n",
    "# Prepare data for modeling\n",
    "\n",
    "# Option 1: Model with rows containing missing values removed\n",
    "df_dropna = df.dropna().copy()\n",
    "X_dropna = df_dropna[['Feature_A', 'Feature_B', 'Feature_C']]\n",
    "y_dropna = df_dropna['Target']\n",
    "X_train_dropna, X_test_dropna, y_train_dropna, y_test_dropna = train_test_split(X_dropna, y_dropna, test_size=0.3, random_state=42)\n",
    "\n",
    "model_dropna = LinearRegression()\n",
    "if not X_train_dropna.empty:\n",
    "    model_dropna.fit(X_train_dropna, y_train_dropna)\n",
    "    if not X_test_dropna.empty:\n",
    "        y_pred_dropna = model_dropna.predict(X_test_dropna)\n",
    "        mse_dropna = mean_squared_error(y_test_dropna, y_pred_dropna)\n",
    "        print(f\"\\nModel performance (missing values dropped): Mean Squared Error = {mse_dropna:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough data to test model after dropping missing values.\")\n",
    "else:\n",
    "    print(\"\\nNot enough data to train model after dropping missing values.\")\n",
    "\n",
    "# Option 2: Model with missing values imputed (using mean imputation)\n",
    "X_imputed_mean = df_imputed_mean[['Feature_A', 'Feature_B', 'Feature_C']]\n",
    "y_imputed_mean = df_imputed_mean['Target']\n",
    "X_train_imputed_mean, X_test_imputed_mean, y_train_imputed_mean, y_test_imputed_mean = train_test_split(X_imputed_mean, y_imputed_mean, test_size=0.3, random_state=42)\n",
    "\n",
    "model_imputed_mean = LinearRegression()\n",
    "model_imputed_mean.fit(X_train_imputed_mean, y_train_imputed_mean)\n",
    "y_pred_imputed_mean = model_imputed_mean.predict(X_test_imputed_mean)\n",
    "mse_imputed_mean = mean_squared_error(y_test_imputed_mean, y_pred_imputed_mean)\n",
    "print(f\"Model performance (missing values imputed with mean): Mean Squared Error = {mse_imputed_mean:.2f}\")\n",
    "\n",
    "# Option 3: Model with missing values imputed (using mode imputation)\n",
    "X_imputed_mode = df_imputed_mode[['Feature_A', 'Feature_B', 'Feature_C']]\n",
    "y_imputed_mode = df_imputed_mode['Target']\n",
    "X_train_imputed_mode, X_test_imputed_mode, y_train_imputed_mode, y_test_imputed_mode = train_test_split(X_imputed_mode, y_imputed_mode, test_size=0.3, random_state=42)\n",
    "\n",
    "model_imputed_mode = LinearRegression()\n",
    "model_imputed_mode.fit(X_train_imputed_mode, y_train_imputed_mode)\n",
    "y_pred_imputed_mode = model_imputed_mode.predict(X_test_imputed_mode)\n",
    "mse_imputed_mode = mean_squared_error(y_test_imputed_mode, y_pred_imputed_mode)\n",
    "print(f\"Model performance (missing values imputed with mode): Mean Squared Error = {mse_imputed_mode:.2f}\")\n",
    "\n",
    "# Option 4: Model using a more sophisticated imputation technique (SimpleImputer with median)\n",
    "df_imputed_median = df.copy()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_imputed_median[['Feature_A', 'Feature_B', 'Feature_C']] = imputer.fit_transform(df_imputed_median[['Feature_A', 'Feature_B', 'Feature_C']])\n",
    "X_imputed_median = df_imputed_median[['Feature_A', 'Feature_B', 'Feature_C']]\n",
    "y_imputed_median = df_imputed_median['Target']\n",
    "X_train_imputed_median, X_test_imputed_median, y_train_imputed_median, y_test_imputed_median = train_test_split(X_imputed_median, y_imputed_median, test_size=0.3, random_state=42)\n",
    "\n",
    "model_imputed_median = LinearRegression()\n",
    "model_imputed_median.fit(X_train_imputed_median, y_train_imputed_median)\n",
    "y_pred_imputed_median = model_imputed_median.predict(X_test_imputed_median)\n",
    "mse_imputed_median = mean_squared_error(y_test_imputed_median, y_pred_imputed_median)\n",
    "print(f\"Model performance (missing values imputed with median): Mean Squared Error = {mse_imputed_median:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Duplicate Data: Repeated data points can skew analysis and model results.\n",
    "#     Task 1: Identify and remove duplicate entries from a dataset using a programming language or tool.\n",
    "#     Task 2: Document the before-and-after dataset shape to understand the impact of duplicates.\n",
    "#     Task 3: Explain to a classmate how duplicate data can affect prediction accuracy.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 1: Identify and remove duplicate entries from a dataset.\n",
      "\n",
      "Original DataFrame:\n",
      "   ID     Name  Age      City\n",
      "0   1    Alice   25  New York\n",
      "1   2      Bob   30    London\n",
      "2   3  Charlie   35     Paris\n",
      "3   4    David   28    London\n",
      "4   2      Bob   30    London\n",
      "5   5      Eve   22     Tokyo\n",
      "6   1    Alice   25  New York\n",
      "7   6    Frank   40    Berlin\n",
      "\n",
      "Duplicate rows:\n",
      "   ID   Name  Age      City\n",
      "4   2    Bob   30    London\n",
      "6   1  Alice   25  New York\n",
      "\n",
      "DataFrame after removing duplicate rows:\n",
      "   ID     Name  Age      City\n",
      "0   1    Alice   25  New York\n",
      "1   2      Bob   30    London\n",
      "2   3  Charlie   35     Paris\n",
      "3   4    David   28    London\n",
      "5   5      Eve   22     Tokyo\n",
      "7   6    Frank   40    Berlin\n",
      "\n",
      "\n",
      "Task 2: Document the before-and-after dataset shape.\n",
      "\n",
      "Shape of the original DataFrame: (8, 4)\n",
      "Shape of the DataFrame after removing duplicates: (6, 4)\n",
      "\n",
      "Number of duplicate rows removed: 2\n",
      "\n",
      "\n",
      "Task 3: Explain how duplicate data can affect prediction accuracy.\n",
      "\n",
      "Hey [Classmate's Name],\n",
      "\n",
      "Let's talk about how duplicate data can mess with our machine learning models and their prediction accuracy. Imagine we're trying to train a model to predict someone's age based on their name and city.\n",
      "\n",
      "If our training data has many identical entries for the same person (same name, same city, same age), the model might get an overinflated sense of how common that specific combination is. It might learn to heavily favor the age associated with those repeated entries, even if that combination isn't actually that frequent in the real world. This can lead to:\n",
      "\n",
      "1. **Bias:** The model can become biased towards the features and target variable of the duplicated data. It might perform very well on data that looks exactly like the duplicates but poorly on slightly different, yet valid, data points.\n",
      "\n",
      "2. **Overfitting:** If the duplicates are essentially the same training example repeated multiple times, the model might start to 'memorize' these specific examples rather than learning the underlying patterns in the data. This overfitting means the model performs well on the training data (including the duplicates) but fails to generalize to new, unseen data.\n",
      "\n",
      "3. **Skewed Evaluation Metrics:** When we evaluate our model, if our test set also contains duplicates that were present in the training data, our performance metrics (like accuracy, precision, recall, or MSE) might appear artificially high. The model is essentially being tested on data it has seen multiple times before.\n",
      "\n",
      "4. **Inefficient Training:** Training on duplicate data is computationally wasteful. The model isn't learning any new information from the repeated entries, but it's still processing them, increasing training time and resource usage.\n",
      "\n",
      "In short, duplicate data can mislead our models into learning spurious correlations, overfit to the training set, and give us a false sense of good performance. Removing duplicates ensures that each data point contributes uniquely to the learning process, leading to more robust and accurate predictions on unseen data.\n",
      "\n",
      "So, it's really important to identify and handle duplicates during our data cleaning process!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 2. Duplicate Data: Repeated data points can skew analysis and model results.\n",
    "\n",
    "# Task 1: Identify and remove duplicate entries from a dataset using a programming language or tool.\n",
    "print(\"\\nTask 1: Identify and remove duplicate entries from a dataset.\")\n",
    "\n",
    "# Create a sample dataset with duplicate rows\n",
    "data = {'ID': [1, 2, 3, 4, 2, 5, 1, 6],\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Bob', 'Eve', 'Alice', 'Frank'],\n",
    "        'Age': [25, 30, 35, 28, 30, 22, 25, 40],\n",
    "        'City': ['New York', 'London', 'Paris', 'London', 'London', 'Tokyo', 'New York', 'Berlin']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Remove duplicate rows, keeping the first occurrence by default\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after removing duplicate rows:\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "# Task 2: Document the before-and-after dataset shape to understand the impact of duplicates.\n",
    "print(\"\\n\\nTask 2: Document the before-and-after dataset shape.\")\n",
    "\n",
    "print(f\"\\nShape of the original DataFrame: {df.shape}\")\n",
    "print(f\"Shape of the DataFrame after removing duplicates: {df_no_duplicates.shape}\")\n",
    "\n",
    "rows_removed = df.shape[0] - df_no_duplicates.shape[0]\n",
    "print(f\"\\nNumber of duplicate rows removed: {rows_removed}\")\n",
    "\n",
    "# Task 3: Explain to a classmate how duplicate data can affect prediction accuracy.\n",
    "print(\"\\n\\nTask 3: Explain how duplicate data can affect prediction accuracy.\")\n",
    "\n",
    "explanation = \"\"\"\n",
    "Hey [Classmate's Name],\n",
    "\n",
    "Let's talk about how duplicate data can mess with our machine learning models and their prediction accuracy. Imagine we're trying to train a model to predict someone's age based on their name and city.\n",
    "\n",
    "If our training data has many identical entries for the same person (same name, same city, same age), the model might get an overinflated sense of how common that specific combination is. It might learn to heavily favor the age associated with those repeated entries, even if that combination isn't actually that frequent in the real world. This can lead to:\n",
    "\n",
    "1. **Bias:** The model can become biased towards the features and target variable of the duplicated data. It might perform very well on data that looks exactly like the duplicates but poorly on slightly different, yet valid, data points.\n",
    "\n",
    "2. **Overfitting:** If the duplicates are essentially the same training example repeated multiple times, the model might start to 'memorize' these specific examples rather than learning the underlying patterns in the data. This overfitting means the model performs well on the training data (including the duplicates) but fails to generalize to new, unseen data.\n",
    "\n",
    "3. **Skewed Evaluation Metrics:** When we evaluate our model, if our test set also contains duplicates that were present in the training data, our performance metrics (like accuracy, precision, recall, or MSE) might appear artificially high. The model is essentially being tested on data it has seen multiple times before.\n",
    "\n",
    "4. **Inefficient Training:** Training on duplicate data is computationally wasteful. The model isn't learning any new information from the repeated entries, but it's still processing them, increasing training time and resource usage.\n",
    "\n",
    "In short, duplicate data can mislead our models into learning spurious correlations, overfit to the training set, and give us a false sense of good performance. Removing duplicates ensures that each data point contributes uniquely to the learning process, leading to more robust and accurate predictions on unseen data.\n",
    "\n",
    "So, it's really important to identify and handle duplicates during our data cleaning process!\n",
    "\"\"\"\n",
    "\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Incorrect Data Types: Data stored in incorrect formats can lead to parsing errors or incorrect analysis.\n",
    "#     Task 1: Convert a column of string numbers to integers in a dataset.\n",
    "#     Task 2: Identify and correct columns with inconsistent data types in a dataset.\n",
    "#     Task 3: Discuss why correct data types are critical for feature engineering.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 1: Convert a column of string numbers to integers.\n",
      "\n",
      "Original DataFrame (Age as strings):\n",
      "   ID     Name Age\n",
      "0   1    Alice  25\n",
      "1   2      Bob  30\n",
      "2   3  Charlie  35\n",
      "3   4    David  28\n",
      "4   5      Eve  40\n",
      "Data type of 'Age' column: object\n",
      "\n",
      "DataFrame after converting 'Age' to integers:\n",
      "   ID     Name  Age\n",
      "0   1    Alice   25\n",
      "1   2      Bob   30\n",
      "2   3  Charlie   35\n",
      "3   4    David   28\n",
      "4   5      Eve   40\n",
      "Data type of 'Age' column: int8\n",
      "\n",
      "\n",
      "Task 2: Identify and correct columns with inconsistent data types.\n",
      "\n",
      "Original DataFrame (with inconsistent data types):\n",
      "   ID  Price Quantity Active\n",
      "0   1  10.50        5   True\n",
      "1   2     20       10  False\n",
      "2   3  15.75        7   TRUE\n",
      "3   4   30.0     12.0  false\n",
      "4   5     25        8   True\n",
      "\n",
      "Data types of each column:\n",
      "ID           int64\n",
      "Price       object\n",
      "Quantity    object\n",
      "Active      object\n",
      "dtype: object\n",
      "\n",
      "'Price' column after conversion to float:\n",
      "0    10.50\n",
      "1    20.00\n",
      "2    15.75\n",
      "3    30.00\n",
      "4    25.00\n",
      "Name: Price, dtype: float64\n",
      "Data type of 'Price' column: float64\n",
      "\n",
      "'Quantity' column after conversion to integer:\n",
      "0     5\n",
      "1    10\n",
      "2     7\n",
      "3    12\n",
      "4     8\n",
      "Name: Quantity, dtype: Int64\n",
      "Data type of 'Quantity' column: Int64\n",
      "\n",
      "'Active' column after conversion to boolean:\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "Name: Active, dtype: bool\n",
      "Data type of 'Active' column: bool\n",
      "\n",
      "DataFrame after correcting inconsistent data types:\n",
      "   ID  Price  Quantity  Active\n",
      "0   1  10.50         5    True\n",
      "1   2  20.00        10    True\n",
      "2   3  15.75         7    True\n",
      "3   4  30.00        12    True\n",
      "4   5  25.00         8    True\n",
      "\n",
      "Corrected data types of each column:\n",
      "ID            int64\n",
      "Price       float64\n",
      "Quantity      Int64\n",
      "Active         bool\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Task 3: Discuss why correct data types are critical for feature engineering.\n",
      "\n",
      "Correct data types are absolutely fundamental for effective feature engineering. Here's why:\n",
      "\n",
      "1. **Mathematical Operations:** Many feature engineering techniques involve mathematical operations between columns (e.g., addition, subtraction, multiplication, division). If a column containing numerical data is stored as a string, these operations will either fail or produce incorrect results (e.g., string concatenation instead of addition). Converting these columns to the appropriate numeric type (integer or float) is essential to perform calculations correctly.\n",
      "\n",
      "2. **Categorical Encoding:** When dealing with categorical features, we often need to encode them into numerical representations that machine learning models can understand (e.g., one-hot encoding, label encoding). These encoding techniques rely on the column being recognized as a categorical type (e.g., 'object' or 'category' in Pandas). If a categorical column is mistakenly identified as a numeric type, encoding will not be applied correctly, leading to errors or suboptimal features. Conversely, trying to encode a numerical column as categorical would also be inappropriate.\n",
      "\n",
      "3. **Date and Time Features:** Feature engineering with date and time data often involves extracting components like day of the week, month, year, hour, time differences, etc. These operations are only possible when the column is correctly recognized as a datetime object in Pandas. If a date or time column is stored as a string, we first need to convert it to datetime using functions like `pd.to_datetime()` before we can extract these meaningful features.\n",
      "\n",
      "4. **Boolean Logic:** Features representing True/False conditions are often used in feature engineering. Ensuring these are stored as boolean data types allows for efficient and correct logical operations and can be crucial for creating indicator variables or applying conditional logic.\n",
      "\n",
      "5. **Type-Specific Functions and Methods:** Pandas and other libraries provide type-specific functions and methods for data manipulation and feature creation. For example, the `.dt` accessor for datetime columns offers a wide range of functionalities for extracting date and time components. Similarly, string-specific methods (`.str`) are available for text data. If the data types are incorrect, these powerful tools cannot be used effectively.\n",
      "\n",
      "6. **Avoiding Unexpected Behavior and Errors:** Incorrect data types can lead to subtle errors in feature engineering that are hard to debug. For instance, comparing a string '10' with an integer 10 might yield unexpected results. Ensuring consistent and correct data types helps prevent such issues and leads to more reliable and predictable feature engineering processes.\n",
      "\n",
      "In summary, having the correct data types ensures that we can apply the appropriate feature engineering techniques, perform calculations accurately, leverage type-specific functionalities, and ultimately create meaningful and effective features that can improve the performance of our machine learning models and the accuracy of our data analysis.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 3. Incorrect Data Types: Data stored in incorrect formats can lead to parsing errors or incorrect analysis.\n",
    "\n",
    "# Task 1: Convert a column of string numbers to integers in a dataset.\n",
    "print(\"\\nTask 1: Convert a column of string numbers to integers.\")\n",
    "\n",
    "# Sample DataFrame with 'Age' as strings\n",
    "data_strings = {'ID': [1, 2, 3, 4, 5],\n",
    "                'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "                'Age': ['25', '30', '35', '28', '40']}\n",
    "df_strings = pd.DataFrame(data_strings)\n",
    "\n",
    "print(\"\\nOriginal DataFrame (Age as strings):\")\n",
    "print(df_strings)\n",
    "print(f\"Data type of 'Age' column: {df_strings['Age'].dtype}\")\n",
    "\n",
    "# Convert 'Age' column to integers\n",
    "df_strings['Age'] = pd.to_numeric(df_strings['Age'], errors='raise', downcast='integer')\n",
    "\n",
    "print(\"\\nDataFrame after converting 'Age' to integers:\")\n",
    "print(df_strings)\n",
    "print(f\"Data type of 'Age' column: {df_strings['Age'].dtype}\")\n",
    "\n",
    "# Task 2: Identify and correct columns with inconsistent data types in a dataset.\n",
    "print(\"\\n\\nTask 2: Identify and correct columns with inconsistent data types.\")\n",
    "\n",
    "# Sample DataFrame with inconsistent data types\n",
    "data_inconsistent = {'ID': [1, 2, 3, 4, 5],\n",
    "                     'Price': ['10.50', '20', '15.75', '30.0', 25],\n",
    "                     'Quantity': ['5', 10, '7', '12.0', 8],\n",
    "                     'Active': ['True', 'False', 'TRUE', 'false', True]}\n",
    "df_inconsistent = pd.DataFrame(data_inconsistent)\n",
    "\n",
    "print(\"\\nOriginal DataFrame (with inconsistent data types):\")\n",
    "print(df_inconsistent)\n",
    "print(\"\\nData types of each column:\")\n",
    "print(df_inconsistent.dtypes)\n",
    "\n",
    "# Correct 'Price' to float\n",
    "df_inconsistent['Price'] = pd.to_numeric(df_inconsistent['Price'], errors='coerce')\n",
    "print(\"\\n'Price' column after conversion to float:\")\n",
    "print(df_inconsistent['Price'])\n",
    "print(f\"Data type of 'Price' column: {df_inconsistent['Price'].dtype}\")\n",
    "\n",
    "# Correct 'Quantity' to integer (handling potential floats)\n",
    "df_inconsistent['Quantity'] = pd.to_numeric(df_inconsistent['Quantity'], errors='coerce').astype('Int64')\n",
    "print(\"\\n'Quantity' column after conversion to integer:\")\n",
    "print(df_inconsistent['Quantity'])\n",
    "print(f\"Data type of 'Quantity' column: {df_inconsistent['Quantity'].dtype}\")\n",
    "\n",
    "# Correct 'Active' to boolean\n",
    "df_inconsistent['Active'] = df_inconsistent['Active'].astype('bool')\n",
    "print(\"\\n'Active' column after conversion to boolean:\")\n",
    "print(df_inconsistent['Active'])\n",
    "print(f\"Data type of 'Active' column: {df_inconsistent['Active'].dtype}\")\n",
    "\n",
    "print(\"\\nDataFrame after correcting inconsistent data types:\")\n",
    "print(df_inconsistent)\n",
    "print(\"\\nCorrected data types of each column:\")\n",
    "print(df_inconsistent.dtypes)\n",
    "\n",
    "# Task 3: Discuss why correct data types are critical for feature engineering.\n",
    "print(\"\\n\\nTask 3: Discuss why correct data types are critical for feature engineering.\")\n",
    "\n",
    "discussion = \"\"\"\n",
    "Correct data types are absolutely fundamental for effective feature engineering. Here's why:\n",
    "\n",
    "1. **Mathematical Operations:** Many feature engineering techniques involve mathematical operations between columns (e.g., addition, subtraction, multiplication, division). If a column containing numerical data is stored as a string, these operations will either fail or produce incorrect results (e.g., string concatenation instead of addition). Converting these columns to the appropriate numeric type (integer or float) is essential to perform calculations correctly.\n",
    "\n",
    "2. **Categorical Encoding:** When dealing with categorical features, we often need to encode them into numerical representations that machine learning models can understand (e.g., one-hot encoding, label encoding). These encoding techniques rely on the column being recognized as a categorical type (e.g., 'object' or 'category' in Pandas). If a categorical column is mistakenly identified as a numeric type, encoding will not be applied correctly, leading to errors or suboptimal features. Conversely, trying to encode a numerical column as categorical would also be inappropriate.\n",
    "\n",
    "3. **Date and Time Features:** Feature engineering with date and time data often involves extracting components like day of the week, month, year, hour, time differences, etc. These operations are only possible when the column is correctly recognized as a datetime object in Pandas. If a date or time column is stored as a string, we first need to convert it to datetime using functions like `pd.to_datetime()` before we can extract these meaningful features.\n",
    "\n",
    "4. **Boolean Logic:** Features representing True/False conditions are often used in feature engineering. Ensuring these are stored as boolean data types allows for efficient and correct logical operations and can be crucial for creating indicator variables or applying conditional logic.\n",
    "\n",
    "5. **Type-Specific Functions and Methods:** Pandas and other libraries provide type-specific functions and methods for data manipulation and feature creation. For example, the `.dt` accessor for datetime columns offers a wide range of functionalities for extracting date and time components. Similarly, string-specific methods (`.str`) are available for text data. If the data types are incorrect, these powerful tools cannot be used effectively.\n",
    "\n",
    "6. **Avoiding Unexpected Behavior and Errors:** Incorrect data types can lead to subtle errors in feature engineering that are hard to debug. For instance, comparing a string '10' with an integer 10 might yield unexpected results. Ensuring consistent and correct data types helps prevent such issues and leads to more reliable and predictable feature engineering processes.\n",
    "\n",
    "In summary, having the correct data types ensures that we can apply the appropriate feature engineering techniques, perform calculations accurately, leverage type-specific functionalities, and ultimately create meaningful and effective features that can improve the performance of our machine learning models and the accuracy of our data analysis.\"\n",
    "\"\"\"\n",
    "\n",
    "print(discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Outliers & Inconsistencies: Irregularities in data can mislead statistical analysis and model predictions.\n",
    "#     Task 1: Visualize a dataset and identify outliers using a boxplot.\n",
    "#     Task 2: Remove or adjust outliers and re-analyze the dataset.\n",
    "#     Task 3: Research and report on a technique for handling outliers effectively.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1240372205.py, line 88)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 88\u001b[0;36m\u001b[0m\n\u001b[0;31m    outlier_handling_report = \"\"\"\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# 4. Outliers & Inconsistencies: Irregularities in data can mislead statistical analysis and model predictions.\n",
    "\n",
    "# Task 1: Visualize a dataset and identify outliers using a boxplot.\n",
    "print(\"\\nTask 1: Visualize a dataset and identify outliers using a boxplot.\")\n",
    "\n",
    "# Create a sample dataset with outliers\n",
    "np.random.seed(42)\n",
    "data_outliers = {'Feature_X': np.concatenate([np.random.normal(loc=20, scale=5, size=100),\n",
    "                                             np.random.normal(loc=50, scale=10, size=10),\n",
    "                                             [-10, 60, 65]])}\n",
    "df_outliers = pd.DataFrame(data_outliers)\n",
    "\n",
    "print(\"\\nOriginal DataFrame with potential outliers:\")\n",
    "print(df_outliers.head())\n",
    "\n",
    "# Visualize using a boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y=df_outliers['Feature_X'])\n",
    "plt.title('Boxplot of Feature_X (Identifying Outliers)')\n",
    "plt.ylabel('Feature_X')\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers based on IQR (Interquartile Range)\n",
    "Q1 = df_outliers['Feature_X'].quantile(0.25)\n",
    "Q3 = df_outliers['Feature_X'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df_outliers[(df_outliers['Feature_X'] < lower_bound) | (df_outliers['Feature_X'] > upper_bound)]\n",
    "print(f\"\\nOutliers identified using IQR method:\")\n",
    "print(outliers_iqr)\n",
    "\n",
    "# Task 2: Remove or adjust outliers and re-analyze the dataset.\n",
    "print(\"\\n\\nTask 2: Remove or adjust outliers and re-analyze the dataset.\")\n",
    "\n",
    "# Option 1: Remove outliers\n",
    "df_no_outliers = df_outliers[(df_outliers['Feature_X'] >= lower_bound) & (df_outliers['Feature_X'] <= upper_bound)].copy()\n",
    "print(\"\\nDataFrame after removing outliers (based on IQR):\")\n",
    "print(df_no_outliers.head())\n",
    "\n",
    "# Re-analyze (e.g., visualize the distribution again)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y=df_no_outliers['Feature_X'])\n",
    "plt.title('Boxplot of Feature_X (Outliers Removed)')\n",
    "plt.ylabel('Feature_X')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_no_outliers['Feature_X'], kde=True)\n",
    "plt.title('Distribution of Feature_X (Outliers Removed)')\n",
    "plt.xlabel('Feature_X')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Option 2: Adjust outliers (Capping/Winsorizing)\n",
    "df_capped = df_outliers.copy()\n",
    "df_capped['Feature_X_Capped'] = np.where(df_capped['Feature_X'] < lower_bound, lower_bound,\n",
    "                                        np.where(df_capped['Feature_X'] > upper_bound, upper_bound,\n",
    "                                                 df_capped['Feature_X']))\n",
    "\n",
    "print(\"\\nDataFrame with outliers capped (based on IQR):\")\n",
    "print(df_capped.head())\n",
    "\n",
    "# Re-analyze the capped data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y=df_capped['Feature_X_Capped'])\n",
    "plt.title('Boxplot of Feature_X (Outliers Capped)')\n",
    "plt.ylabel('Feature_X_Capped')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_capped['Feature_X_Capped'], kde=True)\n",
    "plt.title('Distribution of Feature_X (Outliers Capped)')\n",
    "plt.xlabel('Feature_X_Capped')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Task 3: Research and report on a technique for handling outliers effectively.\n",
    "print(\"\\n\\nTask 3: Research and report on a technique for handling outliers effectively.\")\n",
    "\n",
    "outlier_handling_report = \"\"\"\n",
    "**Technique for Handling Outliers Effectively: Robust Scaling**\n",
    "\n",
    "**Description:**\n",
    "Robust scaling is a technique used to standardize numerical features by removing the median and scaling the data according to the Interquartile Range (IQR). The IQR is the range between the first quartile (Q1) and the third quartile (Q3). This method is less sensitive to outliers compared to standard scaling (which uses mean and standard deviation) because the median and IQR are robust to extreme values.\n",
    "\n",
    "**Formula:**\n",
    "The RobustScaler scales data using the following formula:\n",
    "\n",
    "$$\n",
    "X_{scaled} = \\frac{X - median(X)}{IQR(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the original data point.\n",
    "- $median(X)$ is the median of the feature.\n",
    "- $IQR(X) = Q3(X) - Q1(X)$ is the interquartile range of the feature.\n",
    "\n",
    "**Why it's effective for handling outliers:**\n",
    "\n",
    "1.  **Robust to Outliers:** Unlike mean and standard deviation, the median and IQR are not significantly affected by the presence of outliers. This means that the scaling is driven by the central portion of the data distribution, and outliers do not have a disproportionate influence on the scaling.\n",
    "\n",
    "2.  **Preserves the Spread:** While it scales the data, robust scaling maintains the relative spread and distribution of the non-outlier data points. It focuses on making the bulk of the data have a similar scale.\n",
    "\n",
    "3.  **Useful for Algorithms Sensitive to Feature Scale:** Many machine learning algorithms (e.g., distance-based algorithms like KNN, SVM, and neural networks) are sensitive to the scale of input features. Robust scaling can help to ensure that features with outliers do not dominate the distance calculations or model learning process.\n",
    "\n",
    "**When to use Robust Scaling:**\n",
    "\n",
    "- When your data contains significant outliers.\n",
    "- When you want to scale your data but are concerned that outliers might skew the scaling if you use StandardScaler.\n",
    "- When the underlying distribution of your data is not necessarily Gaussian.\n",
    "\n",
    "**Implementation in Python (using scikit-learn):**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[-1.0], [0.0], [1.0], [2.0], [100.0]])\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(f\"Original data: {data.flatten()}\")\n",
    "print(f\"Robustly scaled data: {scaled_data.flatten()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

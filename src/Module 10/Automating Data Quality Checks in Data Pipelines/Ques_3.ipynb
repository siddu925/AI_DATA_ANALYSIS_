{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Data Quality Monitoring\n",
    "**Objective**: Use Great Expectations to perform data profiling and write validation rules.\n",
    "\n",
    "1. Data Profiling with Great Expectations\n",
    "\n",
    "### Profile a JSON dataset with product sales data to check for null values in the 'ProductID' and 'Price' fields.\n",
    "- Create an expectation suite and connect it to the data context.\n",
    "- Use the `expect_column_values_to_not_be_null` expectation to profile these fields.\n",
    "- Review the summary to identify any unexpected null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'great_expectations' has no attribute 'DataContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Create an Expectation Suite and connect it to the Data Context\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mgx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataContext\u001b[49m()\n\u001b[1;32m      7\u001b[0m expectation_suite_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_sales_null_check_suite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m suite \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcreate_expectation_suite(\n\u001b[1;32m      9\u001b[0m     expectation_suite_name\u001b[38;5;241m=\u001b[39mexpectation_suite_name,\n\u001b[1;32m     10\u001b[0m     overwrite_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'great_expectations' has no attribute 'DataContext'"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "import json\n",
    "\n",
    "# 1. Create an Expectation Suite and connect it to the Data Context\n",
    "context = gx.DataContext()\n",
    "\n",
    "expectation_suite_name = \"product_sales_null_check_suite\"\n",
    "suite = context.create_expectation_suite(\n",
    "    expectation_suite_name=expectation_suite_name,\n",
    "    overwrite_existing=True,\n",
    ")\n",
    "\n",
    "# Assuming your JSON file is in the same directory or provide the full path\n",
    "json_file_path = \"product_sales.json\"  # Replace with your actual JSON file name\n",
    "\n",
    "# Load the JSON data using Python's built-in json library\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Great Expectations can work with various backends. For a quick check on in-memory data,\n",
    "# we can use a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add a Pandas DataFrame Data Source\n",
    "datasource_name = \"my_pandas_datasource\"\n",
    "datasource = context.sources.add_pandas(name=datasource_name)\n",
    "\n",
    "# Create a Data Asset from the Pandas DataFrame\n",
    "data_asset_name = \"product_sales_data\"\n",
    "data_asset = datasource.add_dataframe_asset(name=data_asset_name)\n",
    "\n",
    "batch_request = data_asset.build_batch_request(dataframe=df)\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite=suite,  # Use the suite we just created\n",
    ")\n",
    "\n",
    "print(f\"Created validator for data asset: {validator.active_batch.data_asset.name}\")\n",
    "print(f\"Using Expectation Suite: {validator.expectation_suite.name}\")\n",
    "\n",
    "# 2. Use the expect_column_values_to_not_be_null expectation to profile these fields.\n",
    "validator.expect_column_values_to_not_be_null(column=\"ProductID\")\n",
    "validator.expect_column_values_to_not_be_null(column=\"Price\")\n",
    "\n",
    "# Save the Expectation Suite\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "# 3. Review the summary to identify any unexpected null values.\n",
    "# To see the results, you'll typically run a Checkpoint. Let's configure and run one.\n",
    "\n",
    "checkpoint_name = \"product_sales_null_check_checkpoint\"\n",
    "checkpoint_result = context.run_checkpoint(\n",
    "    checkpoint_name=checkpoint_name,\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": batch_request,\n",
    "            \"expectation_suite_name\": expectation_suite_name,\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "validation_result = checkpoint_result.list_validation_results()[0]\n",
    "for expectation_result in validation_result[\"results\"]:\n",
    "    if expectation_result[\"expectation_config\"][\"expectation_type\"] == \"expect_column_values_to_not_be_null\" and expectation_result[\"expectation_config\"][\"kwargs\"][\"column\"] in [\"ProductID\", \"Price\"]:\n",
    "        print(f\"Column '{expectation_result['expectation_config']['kwargs']['column']}': {expectation_result['success']}\")\n",
    "        if not expectation_result[\"success\"]:\n",
    "            print(f\"  - Unexpected null values found: {expectation_result['result'].get('unexpected_count', 'N/A')}\")\n",
    "            if \"partial_unexpected_list\" in expectation_result[\"result\"]:\n",
    "                print(f\"  - Partial list of unexpected nulls: {expectation_result['result']['partial_unexpected_list']}\")\n",
    "\n",
    "# You can also view a more detailed report in the Data Docs:\n",
    "print(\"\\nTo view the detailed validation report in Data Docs:\")\n",
    "print(f\"- Navigate to your Great Expectations Data Context directory.\")\n",
    "print(\"- Run the command: `great_expectations docs build`\")\n",
    "print(\"- Open the generated `index.html` file and find the results for the '{checkpoint_name}' Checkpoint and the '{expectation_suite_name}' Expectation Suite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Writing Validation Rules for Data Ingestion\n",
    "\n",
    "### Define validation rules for an API data source to confirm that 'Status' field contains only predefined statuses ('Active', 'Inactive').\n",
    "\n",
    "- Apply `expect_column_values_to_be_in_set` to check field values during data ingestion.\n",
    "- Execute the validation and review any mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Create a Data Context (if you don't have one already)\n",
    "context = gx.DataContext()\n",
    "\n",
    "# 2. Define the Expectation Suite name\n",
    "expectation_suite_name = \"api_status_validation_suite\"\n",
    "\n",
    "# Create the Expectation Suite if it doesn't exist\n",
    "try:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name)\n",
    "    print(f\"Loaded existing Expectation Suite: {suite.name}\")\n",
    "except gx.exceptions.ExpectationSuiteNotFoundError:\n",
    "    suite = context.create_expectation_suite(\n",
    "        expectation_suite_name=expectation_suite_name,\n",
    "        overwrite_existing=True,\n",
    "    )\n",
    "    print(f\"Created Expectation Suite: {suite.name}\")\n",
    "\n",
    "# 3. Simulate fetching data from an API (replace with your actual API interaction)\n",
    "api_data = [\n",
    "    {\"UserID\": 1, \"Name\": \"Alice\", \"Status\": \"Active\"},\n",
    "    {\"UserID\": 2, \"Name\": \"Bob\", \"Status\": \"Inactive\"},\n",
    "    {\"UserID\": 3, \"Name\": \"Charlie\", \"Status\": \"Pending\"},\n",
    "    {\"UserID\": 4, \"Name\": \"David\", \"Status\": \"Active\"},\n",
    "    {\"UserID\": 5, \"Name\": \"Eve\", \"Status\": \"Unknown\"},\n",
    "    {\"UserID\": 6, \"Name\": \"Frank\", \"Status\": \"Inactive\"},\n",
    "]\n",
    "\n",
    "# Convert the API data to a Pandas DataFrame (a common way to work with data in GE)\n",
    "df = pd.DataFrame(api_data)\n",
    "\n",
    "# 4. Add a Pandas DataFrame Data Source and Data Asset\n",
    "datasource_name = \"api_data_source\"\n",
    "datasource = context.sources.add_pandas(name=datasource_name)\n",
    "\n",
    "data_asset_name = \"api_data\"\n",
    "data_asset = datasource.add_dataframe_asset(name=data_asset_name)\n",
    "\n",
    "batch_request = data_asset.build_batch_request(dataframe=df)\n",
    "\n",
    "# 5. Get a Validator\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite=suite,\n",
    ")\n",
    "\n",
    "print(f\"Using Expectation Suite: {validator.expectation_suite.name}\")\n",
    "\n",
    "# 6. Apply the expect_column_values_to_be_in_set expectation\n",
    "allowed_statuses = [\"Active\", \"Inactive\"]\n",
    "validator.expect_column_values_to_be_in_set(\n",
    "    column=\"Status\",\n",
    "    value_set=allowed_statuses,\n",
    "    mostly=1.0,  # Optional: You can adjust this if a certain percentage of invalid values is acceptable\n",
    ")\n",
    "\n",
    "# 7. Save the Expectation Suite\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "# 8. Execute the validation and review any mismatches using a Checkpoint\n",
    "checkpoint_name = \"api_data_validation_checkpoint\"\n",
    "checkpoint_result = context.run_checkpoint(\n",
    "    checkpoint_name=checkpoint_name,\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": batch_request,\n",
    "            \"expectation_suite_name\": expectation_suite_name,\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 9. Review the validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "validation_result = checkpoint_result.list_validation_results()[0]\n",
    "for expectation_result in validation_result[\"results\"]:\n",
    "    if expectation_result[\"expectation_config\"][\"expectation_type\"] == \"expect_column_values_to_be_in_set\" and expectation_result[\"expectation_config\"][\"kwargs\"][\"column\"] == \"Status\":\n",
    "        print(f\"Expectation for 'Status' column: {expectation_result['success']}\")\n",
    "        if not expectation_result[\"success\"]:\n",
    "            print(f\"  - Unexpected values found: {expectation_result['result'].get('unexpected_count', 'N/A')}\")\n",
    "            if \"partial_unexpected_list\" in expectation_result[\"result\"]:\n",
    "                print(f\"  - Partial list of unexpected values: {expectation_result['result']['partial_unexpected_list']}\")\n",
    "\n",
    "# 10. Optionally, view the detailed report in Data Docs\n",
    "print(\"\\nTo view the detailed validation report in Data Docs:\")\n",
    "print(f\"- Navigate to your Great Expectations Data Context directory.\")\n",
    "print(\"- Run the command: `great_expectations docs build`\")\n",
    "print(\"- Open the generated `index.html` file and find the results for the '{checkpoint_name}' Checkpoint and the '{expectation_suite_name}' Expectation Suite.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

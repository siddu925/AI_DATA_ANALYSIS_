{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Deduplication using Clustering\n",
    "**Objective**: Learn and implement data deduplication techniques.\n",
    "\n",
    "**Task**: Deduplication Using K-means Clustering\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Download a dataset containing duplicate customer records.\n",
    "2. Preprocess: Standardize the data to ensure better clustering.\n",
    "3. Apply K-means: Use K-means clustering to find and group similar customer records.\n",
    "4. Identify Duplicates: Identify and remove duplicates within clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Potential Duplicates:\n",
      "   CustomerID        Name  Age  Income       City\n",
      "0           1  Customer 1   58  119135     Mumbai\n",
      "1           2  Customer 2   48   65222  Bengaluru\n",
      "2           3  Customer 3   34  107373     Mumbai\n",
      "3           4  Customer 4   27  109575    Chennai\n",
      "4           5  Customer 5   40  126354    Chennai\n",
      "\n",
      "Processed DataFrame (Scaled Numerical Features):\n",
      "   CustomerID        Name       City       Age    Income\n",
      "0           1  Customer 1     Mumbai  1.685131  0.680862\n",
      "1           2  Customer 2  Bengaluru  0.809196 -0.845697\n",
      "2           3  Customer 3     Mumbai -0.417112  0.347818\n",
      "3           4  Customer 4    Chennai -1.030265  0.410168\n",
      "4           5  Customer 5    Chennai  0.108449  0.885270\n",
      "\n",
      "DataFrame with Cluster Assignments:\n",
      "   CustomerID        Name       City       Age    Income  Cluster\n",
      "0           1  Customer 1     Mumbai  1.685131  0.680862        9\n",
      "1           2  Customer 2  Bengaluru  0.809196 -0.845697        8\n",
      "2           3  Customer 3     Mumbai -0.417112  0.347818        2\n",
      "3           4  Customer 4    Chennai -1.030265  0.410168        2\n",
      "4           5  Customer 5    Chennai  0.108449  0.885270       14\n",
      "\n",
      "Deduplicated DataFrame:\n",
      "   CustomerID        Name  Age  Income       City  Cluster\n",
      "0           1  Customer 1   58  119135     Mumbai        9\n",
      "1           2  Customer 2   48   65222  Bengaluru        8\n",
      "2           3  Customer 3   34  107373     Mumbai        2\n",
      "3           5  Customer 5   40  126354    Chennai       14\n",
      "4           7  Customer 7   38   93335      Delhi        7\n",
      "\n",
      "Number of original records: 105\n",
      "Number of deduplicated records: 15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Step 1: Generate a dataset containing duplicate customer records (replace with your actual data)\n",
    "np.random.seed(42)\n",
    "num_records = 100\n",
    "data = {\n",
    "    'CustomerID': np.arange(1, num_records + 1),\n",
    "    'Name': [f'Customer {i}' for i in range(1, num_records + 1)],\n",
    "    'Age': np.random.randint(20, 60, num_records),\n",
    "    'Income': np.random.randint(30000, 150000, num_records),\n",
    "    'City': np.random.choice(['Bengaluru', 'Mumbai', 'Delhi', 'Chennai'], num_records)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some duplicates with slight variations\n",
    "df_duplicates = pd.DataFrame({\n",
    "    'CustomerID': [101, 102, 103, 104, 105],\n",
    "    'Name': ['Customer 5', 'Custumer 15', 'Customer 32', 'Customer 67', 'Customer 88'], # Slight typos\n",
    "    'Age': [25, 38, 45, 52, 31],\n",
    "    'Income': [62000, 91000, 78000, 125000, 48000],\n",
    "    'City': ['Bengaluru', 'Mumbai', 'Delhi', 'Chennai', 'Bengaluru']\n",
    "})\n",
    "\n",
    "df = pd.concat([df, df_duplicates], ignore_index=True)\n",
    "np.random.shuffle(df.values) # Shuffle the order\n",
    "\n",
    "print(\"Original DataFrame with Potential Duplicates:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Preprocess: Standardize the data to ensure better clustering.\n",
    "# Select numerical features for clustering\n",
    "numerical_features = ['Age', 'Income']\n",
    "X = df[numerical_features].copy()\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=numerical_features)\n",
    "\n",
    "# Combine scaled numerical features with categorical features (for later identification)\n",
    "df_processed = pd.concat([df[['CustomerID', 'Name', 'City']].reset_index(drop=True), X_scaled_df], axis=1)\n",
    "print(\"\\nProcessed DataFrame (Scaled Numerical Features):\")\n",
    "print(df_processed.head())\n",
    "\n",
    "# Step 3: Apply K-means: Use K-means clustering to find and group similar customer records.\n",
    "# Determine the optimal number of clusters (you might need to experiment with this)\n",
    "# For demonstration, let's assume we expect a few duplicate groups, so we'll choose a small number of clusters.\n",
    "n_clusters = 15\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df_processed['Cluster'] = kmeans.fit_predict(df_processed[numerical_features])\n",
    "\n",
    "print(\"\\nDataFrame with Cluster Assignments:\")\n",
    "print(df_processed.head())\n",
    "\n",
    "# Step 4: Identify Duplicates: Identify and remove duplicates within clusters.\n",
    "def identify_and_remove_duplicates(cluster_df):\n",
    "    if len(cluster_df) <= 1:\n",
    "        return cluster_df\n",
    "    # Calculate pairwise distances based on the scaled numerical features\n",
    "    distances = pairwise_distances(cluster_df[numerical_features], metric='euclidean')\n",
    "    # Set a threshold for considering records as duplicates (you might need to tune this)\n",
    "    # A smaller threshold means records need to be very similar to be considered duplicates\n",
    "    similarity_threshold = 1.0\n",
    "    duplicates_to_drop = set()\n",
    "    for i in range(len(cluster_df)):\n",
    "        for j in range(i + 1, len(cluster_df)):\n",
    "            if distances[i, j] < similarity_threshold:\n",
    "                # Keep the record with the lower CustomerID as the representative\n",
    "                index_to_drop = cluster_df.iloc[j].name\n",
    "                duplicates_to_drop.add(index_to_drop)\n",
    "    return cluster_df.drop(index=duplicates_to_drop)\n",
    "\n",
    "# Group by cluster and apply the duplicate removal function\n",
    "df_deduplicated = df_processed.groupby('Cluster', group_keys=False).apply(identify_and_remove_duplicates).reset_index(drop=True)\n",
    "\n",
    "# Merge back with the original non-scaled data to show all columns\n",
    "df_final = pd.merge(df[['CustomerID', 'Name', 'Age', 'Income', 'City']],\n",
    "                    df_deduplicated[['CustomerID', 'Cluster']],\n",
    "                    on='CustomerID', how='inner')\n",
    "\n",
    "print(\"\\nDeduplicated DataFrame:\")\n",
    "print(df_final.head())\n",
    "print(f\"\\nNumber of original records: {len(df)}\")\n",
    "print(f\"Number of deduplicated records: {len(df_final.drop_duplicates(subset=['Name', 'Age', 'Income', 'City']))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

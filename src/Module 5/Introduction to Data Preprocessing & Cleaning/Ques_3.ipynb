{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps in Data Preprocessing\n",
    "\n",
    "# 1. Data Collection: Gathering raw data from various sources.\n",
    "# Task 1: Collect data from two different sources and merge them.\n",
    "# Task 2: Validate the integrity of the collected datasets.\n",
    "# Task 3: Reflect on challenges faced during data collection and how they were addressed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1380038456.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 58\u001b[0;36m\u001b[0m\n\u001b[0;31m    reflection = \"\"\"\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Data Collection: Gathering raw data from various sources.\n",
    "\n",
    "# Task 1: Collect data from two different sources and merge them.\n",
    "print(\"\\nTask 1: Collect data from two different sources and merge them.\")\n",
    "\n",
    "# Simulate data from Source A (e.g., a CSV file)\n",
    "data_source_a = {'ID': [1, 2, 3, 4, 5],\n",
    "                  'Product': ['Laptop', 'Tablet', 'Keyboard', 'Mouse', 'Monitor'],\n",
    "                  'Price': [1200, 300, 75, 25, 250]}\n",
    "df_a = pd.DataFrame(data_source_a)\n",
    "print(\"\\nData from Source A:\")\n",
    "print(df_a)\n",
    "\n",
    "# Simulate data from Source B (e.g., an API response or another CSV)\n",
    "data_source_b = {'ID': [3, 4, 5, 6, 7],\n",
    "                  'Quantity_Sold': [100, 150, 200, 50, 120],\n",
    "                  'Customer_Rating': [4.5, 4.2, 4.8, 3.9, 4.6]}\n",
    "df_b = pd.DataFrame(data_source_b)\n",
    "print(\"\\nData from Source B:\")\n",
    "print(df_b)\n",
    "\n",
    "# Merge the two DataFrames based on a common key ('ID')\n",
    "merged_df = pd.merge(df_a, df_b, on='ID', how='inner')\n",
    "print(\"\\nMerged DataFrame (inner join on 'ID'):\")\n",
    "print(merged_df)\n",
    "\n",
    "# Task 2: Validate the integrity of the collected datasets.\n",
    "print(\"\\n\\nTask 2: Validate the integrity of the collected datasets.\")\n",
    "\n",
    "print(\"\\nIntegrity checks for Source A:\")\n",
    "print(f\"Number of rows: {len(df_a)}\")\n",
    "print(f\"Number of unique IDs: {df_a['ID'].nunique()}\")\n",
    "print(f\"Data types:\\n{df_a.dtypes}\")\n",
    "print(f\"Missing values:\\n{df_a.isnull().sum()}\")\n",
    "\n",
    "print(\"\\nIntegrity checks for Source B:\")\n",
    "print(f\"Number of rows: {len(df_b)}\")\n",
    "print(f\"Number of unique IDs: {df_b['ID'].nunique()}\")\n",
    "print(f\"Data types:\\n{df_b.dtypes}\")\n",
    "print(f\"Missing values:\\n{df_b.isnull().sum()}\")\n",
    "\n",
    "print(\"\\nIntegrity checks for Merged DataFrame:\")\n",
    "print(f\"Number of rows: {len(merged_df)}\")\n",
    "print(f\"Number of unique IDs: {merged_df['ID'].nunique()}\")\n",
    "print(f\"Data types:\\n{merged_df.dtypes}\")\n",
    "print(f\"Missing values:\\n{merged_df.isnull().sum()}\")\n",
    "\n",
    "# Further validation examples:\n",
    "# Check for consistent data types across common columns (after potential initial loading)\n",
    "# Check for expected value ranges (e.g., price should not be negative)\n",
    "# Check for logical inconsistencies between columns\n",
    "\n",
    "# Task 3: Reflect on challenges faced during data collection and how they were addressed.\n",
    "print(\"\\n\\nTask 3: Reflect on challenges faced during data collection and how they were addressed.\")\n",
    "\n",
    "reflection = \"\"\"\n",
    "Reflecting on potential challenges during data collection and how they might be addressed:\n",
    "\n",
    "**Challenge 1: Different Data Formats:**\n",
    "- **Description:** Data might come in various formats (CSV, JSON, Excel, databases, APIs).\n",
    "- **Address:** Use appropriate libraries in Python (e.g., pandas for tabular data, json for JSON, requests for APIs) to read and parse data from different sources. Ensure consistent parsing and handling of data types during the loading process.\n",
    "\n",
    "**Challenge 2: Inconsistent Schemas:**\n",
    "- **Description:** The structure (column names, order) of data might differ between sources, even if they contain similar information.\n",
    "- **Address:** Before merging, standardize column names (e.g., convert to lowercase, replace spaces). Select and align relevant columns. You might need to rename columns in one or both DataFrames before merging to ensure the join key and other relevant columns have the same names.\n",
    "\n",
    "**Challenge 3: Data Integrity Issues in Sources:**\n",
    "- **Description:** Individual data sources might contain errors, missing values, or inconsistencies (e.g., different units, typos).\n",
    "- **Address:** Perform initial validation on each source *before* merging. Identify and flag or handle missing values (imputation, removal). Standardize units and correct obvious errors. This early cleaning can prevent propagation of issues in the merged dataset.\n",
    "\n",
    "**Challenge 4: Handling Different Granularity:**\n",
    "- **Description:** Data from different sources might be at different levels of detail (e.g., one source has daily sales, another has monthly).\n",
    "- **Address:** Decide on the desired level of granularity for your analysis. You might need to aggregate data from a finer granularity source or disaggregate (with caution and assumptions) from a coarser one before merging.\n",
    "\n",
    "**Challenge 5: Ensuring Unique Identifiers for Merging:**\n",
    "- **Description:** Merging relies on common identifiers. If these identifiers are inconsistent or not truly unique across sources, the merge can produce incorrect results (e.g., many-to-many joins when one-to-one is expected).\n",
    "- **Address:** Understand the nature of the identifiers in each source. You might need to create a composite key from multiple columns to ensure uniqueness or perform careful analysis of the join keys to identify and resolve inconsistencies before merging.\n",
    "\n",
    "**Challenge 6: Authentication and Access Issues:**\n",
    "- **Description:** Accessing data from APIs or databases might require authentication, and there could be rate limits or access restrictions.\n",
    "- **Address:** Implement proper authentication mechanisms. Handle API rate limits with delays or batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning: Addressing missing values, duplicates, incorrect types, and outliers.\n",
    "# Task 1: Clean a given dataset and document the changes made.\n",
    "# Task 2: Create a checklist to ensure comprehensive data cleaning in future projects.\n",
    "# Task 3: Collaborate with a peer to clean a new dataset and present your solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# 2. Data Cleaning: Addressing missing values, duplicates, incorrect types, and outliers.\n",
    "\n",
    "# Task 1: Clean a given dataset and document the changes made.\n",
    "print(\"\\nTask 1: Clean a given dataset and document the changes made.\")\n",
    "\n",
    "# Sample dataset with various cleaning needs\n",
    "data_dirty = {'ID': [1, 2, 3, 4, 2, 5, 6, 1, 7, 8],\n",
    "              'Name': ['Alice', 'bob', 'Charlie', np.nan, 'Eve', 'BOB', 'Frank', 'Alice', 'Grace', 'Harry'],\n",
    "              'Age': ['25', '30.0', 35, '28', '40', '30', '45', '25', np.nan, -5],\n",
    "              'Salary': [50000, 60000, np.nan, 70000, 80000, 60000, '55000', 90000, 75000, 800000],\n",
    "              'City': ['New York', 'london', 'Paris', 'London', 'tokyo', 'london', 'Berlin', 'new york', 'Sydney', 'Bengaluru'],\n",
    "              'Enrollment_Date': ['2023-01-15', '2023-02-20', '2023-03-10', '2023-04-05', '2023-05-01',\n",
    "                                  '2023-02-20', '2023-06-12', '2023-01-15', '2023-07-21', '2023-08-30']}\n",
    "df_dirty = pd.DataFrame(data_dirty)\n",
    "\n",
    "print(\"\\nOriginal Dirty DataFrame:\")\n",
    "print(df_dirty)\n",
    "print(f\"\\nOriginal DataFrame Info:\\n{df_dirty.info()}\")\n",
    "\n",
    "cleaning_log = []\n",
    "\n",
    "# --- Step 1: Handle Duplicates ---\n",
    "initial_shape = df_dirty.shape\n",
    "df_cleaned = df_dirty.drop_duplicates()\n",
    "duplicates_removed = initial_shape[0] - df_cleaned.shape[0]\n",
    "cleaning_log.append(f\"Removed {duplicates_removed} duplicate rows.\")\n",
    "\n",
    "# --- Step 2: Standardize Text Data ---\n",
    "df_cleaned['Name'] = df_cleaned['Name'].str.lower()\n",
    "df_cleaned['City'] = df_cleaned['City'].str.lower()\n",
    "cleaning_log.append(\"Standardized 'Name' and 'City' columns to lowercase.\")\n",
    "\n",
    "# --- Step 3: Handle Missing Values ---\n",
    "missing_before = df_cleaned.isnull().sum()\n",
    "df_cleaned['Name'].fillna('unknown', inplace=True)\n",
    "df_cleaned['Age'].fillna(df_cleaned['Age'].median(), inplace=True)\n",
    "df_cleaned['Salary'].fillna(df_cleaned['Salary'].median(), inplace=True)\n",
    "missing_after = df_cleaned.isnull().sum()\n",
    "cleaning_log.append(f\"Filled missing 'Name' with 'unknown', 'Age' with median ({df_cleaned['Age'].median()}), and 'Salary' with median ({df_cleaned['Salary'].median()}).\")\n",
    "\n",
    "# --- Step 4: Correct Data Types ---\n",
    "df_cleaned['Age'] = pd.to_numeric(df_cleaned['Age'], errors='coerce').astype('Int64')\n",
    "df_cleaned['Salary'] = pd.to_numeric(df_cleaned['Salary'], errors='coerce')\n",
    "df_cleaned['Enrollment_Date'] = pd.to_datetime(df_cleaned['Enrollment_Date'], errors='coerce')\n",
    "cleaning_log.append(\"Converted 'Age' to integer, 'Salary' to numeric, and 'Enrollment_Date' to datetime. Invalid conversions resulted in NaT/NaN.\")\n",
    "\n",
    "# --- Step 5: Handle Outliers (Age) ---\n",
    "age_mean = df_cleaned['Age'].mean()\n",
    "age_std = df_cleaned['Age'].std()\n",
    "age_threshold = 3\n",
    "outliers_age = df_cleaned[(df_cleaned['Age'] < age_mean - age_threshold * age_std) | (df_cleaned['Age'] > age_mean + age_threshold * age_std)]\n",
    "df_cleaned['Age'] = np.where(df_cleaned['Age'] < 0, np.nan, df_cleaned['Age']) # Correcting illogical negative age\n",
    "df_cleaned['Age'].fillna(df_cleaned['Age'].median(), inplace=True) # Re-impute any NaNs from illogical values\n",
    "cleaning_log.append(f\"Handled illogical negative values in 'Age' by setting them to NaN and then re-imputing with the median.\")\n",
    "\n",
    "# --- Step 6: Handle Outliers (Salary) ---\n",
    "salary_mean = df_cleaned['Salary'].mean()\n",
    "salary_std = df_cleaned['Salary'].std()\n",
    "salary_threshold = 3\n",
    "outliers_salary = df_cleaned[(df_cleaned['Salary'] < salary_mean - salary_threshold * salary_std) | (df_cleaned['Salary'] > salary_mean + salary_threshold * salary_std)]\n",
    "# For demonstration, let's cap outliers instead of removing\n",
    "lower_salary_bound = salary_mean - salary_threshold * salary_std\n",
    "upper_salary_bound = salary_mean + salary_threshold * salary_std\n",
    "df_cleaned['Salary'] = np.clip(df_cleaned['Salary'], lower_salary_bound, upper_salary_bound)\n",
    "cleaning_log.append(f\"Capped outliers in 'Salary' to the range [{lower_salary_bound:.2f}, {upper_salary_bound:.2f}].\")\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned)\n",
    "print(f\"\\nCleaned DataFrame Info:\\n{df_cleaned.info()}\")\n",
    "\n",
    "print(\"\\nCleaning Log:\")\n",
    "for log_entry in cleaning_log:\n",
    "    print(f\"- {log_entry}\")\n",
    "\n",
    "# Task 2: Create a checklist to ensure comprehensive data cleaning in future projects.\n",
    "print(\"\\n\\nTask 2: Create a checklist for comprehensive data cleaning.\")\n",
    "\n",
    "data_cleaning_checklist = [\n",
    "    \"Understand the data: Source, meaning of columns, expected data types.\",\n",
    "    \"Identify missing values: Determine the extent and patterns of missing data.\",\n",
    "    \"Handle missing values: Choose appropriate imputation or removal strategies (document the choice).\",\n",
    "    \"Identify duplicate data: Check for and quantify duplicate rows.\",\n",
    "    \"Handle duplicate data: Remove duplicates, deciding which occurrences to keep (document the decision).\",\n",
    "    \"Standardize text data: Ensure consistent casing, remove extra whitespace, handle special characters if needed.\",\n",
    "    \"Correct data types: Verify and convert columns to the appropriate data types (numeric, string, datetime, boolean).\",\n",
    "    \"Identify outliers: Use visualization (boxplots, scatter plots) and statistical methods (Z-score, IQR) to detect outliers.\",\n",
    "    \"Handle outliers: Choose appropriate strategies (removal, capping, transformation) based on the nature of the outliers and the analysis goals (document the choice).\",\n",
    "    \"Address inconsistencies: Look for and resolve inconsistencies in data values (e.g., different units, contradictory entries).\",\n",
    "    \"Validate data integrity: Perform checks for logical errors and data range validity.\",\n",
    "    \"Document all cleaning steps: Maintain a log of changes made and the reasoning behind them.\",\n",
    "    \"Review and iterate: After initial cleaning, review the data for any remaining issues and iterate as needed.\"\n",
    "]\n",
    "\n",
    "print(\"\\nData Cleaning Checklist:\")\n",
    "for item in data_cleaning_checklist:\n",
    "    print(f\"- {item}\")\n",
    "\n",
    "# Task 3: Collaborate with a peer to clean a new dataset and present your solutions.\n",
    "print(\"\\n\\nTask 3: Collaborate with a peer to clean a new dataset and present your solutions.\")\n",
    "\n",
    "print(\"\\n(Imagine collaborating with a peer on a new dataset here.)\")\n",
    "print(\"\\nFor the purpose of this exercise, let's assume we collaborated on a dataset (not provided here) and our combined solution involved:\")\n",
    "collaboration_summary = [\n",
    "    \"Identified and removed 15 duplicate entries based on 'User_ID' and 'Timestamp'.\",\n",
    "    \"Imputed missing 'Review_Score' (numerical) with the median.\",\n",
    "    \"Standardized the 'Product_Category' column to have consistent capitalization.\",\n",
    "    \"Converted the 'Order_Date' column to datetime objects.\",\n",
    "    \"Identified potential outliers in 'Transaction_Amount' using the IQR method and decided to cap them to the 1st and 99th percentiles to retain the data while reducing the impact of extreme values.\",\n",
    "    \"Documented each step and the reasoning behind the chosen methods.\"\n",
    "]\n",
    "\n",
    "print(\"\\nSummary of Collaborative Data Cleaning:\")\n",
    "for item in collaboration_summary:\n",
    "    print(f\"- {item}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Task 1: Clean a Given Dataset and Document Changes**\n",
    "\n",
    "1.  **Load Dirty Data:** We start with a sample Pandas DataFrame (`df_dirty`) containing various data quality issues: duplicates, inconsistent text, missing values, incorrect data types, and outliers.\n",
    "2.  **Cleaning Log:** We initialize an empty list `cleaning_log` to record each cleaning step performed.\n",
    "3.  **Handle Duplicates:** We use `drop_duplicates()` to remove duplicate rows and record the number of duplicates removed.\n",
    "4.  **Standardize Text:** We convert the 'Name' and 'City' columns to lowercase for consistency.\n",
    "5.  **Handle Missing Values:** We identify missing values using `isnull().sum()` and then fill them using different strategies: 'Name' with 'unknown', 'Age' with the median, and 'Salary' with the median. We record the imputation strategies.\n",
    "6.  **Correct Data Types:** We use `pd.to_numeric()` to convert 'Age' and 'Salary' to numeric types and `pd.to_datetime()` for 'Enrollment\\_Date'. We note that invalid conversions will result in `NaN` or `NaT`.\n",
    "7.  **Handle Outliers (Age):** We identify potential outliers in 'Age' using the Z-score method. We also correct illogical negative age values by setting them to `NaN` and then re-imputing.\n",
    "8.  **Handle Outliers (Salary):** We identify potential outliers in 'Salary' using the Z-score method and then demonstrate capping (winsorizing) the outliers to a range defined by the mean and 3 standard deviations.\n",
    "9.  **Print Cleaned Data and Log:** We display the cleaned DataFrame and iterate through the `cleaning_log` to show all the steps taken.\n",
    "\n",
    "**Task 2: Create a Data Cleaning Checklist**\n",
    "\n",
    "We create a comprehensive checklist (`data_cleaning_checklist`) outlining the essential steps involved in data cleaning for future projects. This checklist serves as a guide to ensure a thorough and systematic approach to data preparation.\n",
    "\n",
    "**Task 3: Collaborate with a Peer**\n",
    "\n",
    "This task is designed to be interactive. For the purpose of this solo exercise, we simulate a collaboration scenario. The output provides a placeholder indicating that this would involve working with another person on a new dataset. We then present a `collaboration_summary` that outlines the hypothetical steps and decisions made during such a collaborative cleaning effort. This emphasizes the importance of communication, shared understanding, and consistent documentation when working in a team.\n",
    "\n",
    "This comprehensive example demonstrates the key aspects of data cleaning, from identifying and addressing various data quality issues to documenting the process and considering collaborative efforts. A well-cleaned dataset is crucial for reliable and meaningful data analysis and machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Transformation: Modifying data to fit specific analytical requirements.\n",
    "# Task 1: Transform a date column into separate 'day', 'month', and 'year' columns.\n",
    "# Task 2: Apply normalization to a dataset feature and confirm the changes.\n",
    "# Task 3: Discuss the importance of data transformation in model interpretability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 3. Data Transformation: Modifying data to fit specific analytical requirements.\n",
    "\n",
    "# Task 1: Transform a date column into separate 'day', 'month', and 'year' columns.\n",
    "print(\"\\nTask 1: Transform a date column into separate 'day', 'month', and 'year' columns.\")\n",
    "\n",
    "# Sample DataFrame with a date column\n",
    "data_dates = {'ID': [1, 2, 3, 4, 5],\n",
    "              'Enrollment_Date': ['2023-01-15', '2023-02-20', '2023-03-10', '2024-04-05', '2024-05-01']}\n",
    "df_dates = pd.DataFrame(data_dates)\n",
    "\n",
    "print(\"\\nOriginal DataFrame with 'Enrollment_Date':\")\n",
    "print(df_dates)\n",
    "print(f\"\\nData type of 'Enrollment_Date': {df_dates['Enrollment_Date'].dtype}\")\n",
    "\n",
    "# Convert 'Enrollment_Date' to datetime objects\n",
    "df_dates['Enrollment_Date'] = pd.to_datetime(df_dates['Enrollment_Date'])\n",
    "print(f\"\\nData type of 'Enrollment_Date' after conversion: {df_dates['Enrollment_Date'].dtype}\")\n",
    "\n",
    "# Extract day, month, and year\n",
    "df_dates['Enrollment_Day'] = df_dates['Enrollment_Date'].dt.day\n",
    "df_dates['Enrollment_Month'] = df_dates['Enrollment_Date'].dt.month\n",
    "df_dates['Enrollment_Year'] = df_dates['Enrollment_Date'].dt.year\n",
    "\n",
    "print(\"\\nDataFrame with extracted 'Day', 'Month', and 'Year':\")\n",
    "print(df_dates)\n",
    "\n",
    "# Task 2: Apply normalization to a dataset feature and confirm the changes.\n",
    "print(\"\\n\\nTask 2: Apply normalization to a dataset feature and confirm the changes.\")\n",
    "\n",
    "# Sample DataFrame with a numerical feature to normalize\n",
    "data_normalize = {'Feature_A': [10, 20, 30, 40, 50]}\n",
    "df_normalize = pd.DataFrame(data_normalize)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_normalize)\n",
    "\n",
    "# Initialize MinMaxScaler for normalization (scaling to a range of 0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply normalization\n",
    "df_normalize['Feature_A_Normalized'] = scaler.fit_transform(df_normalize[['Feature_A']])\n",
    "\n",
    "print(\"\\nDataFrame after normalization:\")\n",
    "print(df_normalize)\n",
    "\n",
    "# Confirm the changes by checking the range of the normalized feature\n",
    "print(f\"\\nMinimum value of 'Feature_A_Normalized': {df_normalize['Feature_A_Normalized'].min()}\")\n",
    "print(f\"\\nMaximum value of 'Feature_A_Normalized': {df_normalize['Feature_A_Normalized'].max()}\")\n",
    "\n",
    "# Task 3: Discuss the importance of data transformation in model interpretability.\n",
    "print(\"\\n\\nTask 3: Discuss the importance of data transformation in model interpretability.\")\n",
    "\n",
    "discussion_transformation_interpretability = \"\"\"\n",
    "Data transformation plays a significant role in the interpretability of machine learning models, although the impact can be both positive and negative depending on the specific transformation and the model used.\n",
    "\n",
    "**Positive Impacts on Interpretability:**\n",
    "\n",
    "1.  **Handling Non-Linearity:** Some transformations, like log transformation, can help linearize relationships between features and the target variable. This can make linear models (like linear regression or logistic regression) more effective and their coefficients more directly interpretable. For instance, after a log transformation, a coefficient might represent a percentage change in the target for a unit change in the transformed feature.\n",
    "\n",
    "2.  **Feature Scaling:** Normalization and standardization, by bringing features to a similar scale, can aid in the interpretation of feature importance in models that are sensitive to feature scales, such as distance-based algorithms (e.g., KNN) or models with regularization (e.g., L1/L2 regularized linear models). Without scaling, features with larger ranges might dominate the model, making it harder to assess the true impact of smaller-range features.\n",
    "\n",
    "3.  **Creating More Meaningful Features:** Feature engineering transformations, like extracting date components (day, month, year) or creating interaction terms, can generate new features that have a more direct and intuitive meaning in the context of the problem. These interpretable features can then lead to more understandable model outputs.\n",
    "\n",
    "4.  **Dimensionality Reduction (Indirectly):** Some transformations, like Principal Component Analysis (PCA), aim to reduce the dimensionality of the data while retaining most of the variance. While PCA itself creates new, uncorrelated features that are linear combinations of the original ones (which can be less interpretable), it can simplify the model and sometimes reveal underlying structures that aid in understanding the data's variance.\n",
    "\n",
    "**Negative Impacts or Challenges to Interpretability:**\n",
    "\n",
    "1.  **Loss of Original Units:** Many scaling and normalization techniques remove the original units of the features, making the coefficients in linear models less directly relatable to the real-world impact of the original variables. For example, a coefficient associated with a normalized price is harder to interpret in terms of dollars.\n",
    "\n",
    "2.  **Complexity from Non-Linear Transformations:** While helpful for model performance, complex non-linear transformations (e.g., polynomial features, certain types of encoding for categorical variables) can make the relationship between the original features and the model's output more intricate and harder to explain simply.\n",
    "\n",
    "3.  **Black-Box Transformations:** Some advanced transformations, especially those learned by neural networks in deep learning, can create highly abstract features that are very difficult for humans to understand. This contributes to the \"black-box\" nature of these models.\n",
    "\n",
    "4.  **Interaction Effects:** While creating interaction terms can capture complex relationships, interpreting the combined effect of multiple interacting features can be challenging.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The impact of data transformation on model interpretability is a trade-off. While transformations can improve model performance and sometimes create more meaningful features, they can also obscure the original meaning of the data and introduce complexity. The choice of transformation should consider not only its effect on model accuracy but also the need for interpretability in the specific application. If interpretability is paramount, simpler transformations or techniques that preserve the original feature meaning might be preferred, even if they slightly compromise performance. It's often beneficial to document transformations clearly and, where possible, relate the transformed features back to their original context during interpretation.\n",
    "\"\"\"\n",
    "\n",
    "print(discussion_transformation_interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Scaling: Adjusting data features to a common scale.\n",
    "# Task 1: Apply Min-Max scaling to a dataset.\n",
    "# Task 2: Standardize a dataset and visualize the changes with a histogram.\n",
    "# Task 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 4. Feature Scaling: Adjusting data features to a common scale.\n",
    "\n",
    "# Task 1: Apply Min-Max scaling to a dataset.\n",
    "print(\"\\nTask 1: Apply Min-Max scaling to a dataset.\")\n",
    "\n",
    "# Sample DataFrame with numerical features\n",
    "data_scaling = {'Feature_A': [10, 20, 30, 40, 50],\n",
    "                'Feature_B': [100, 50, 200, 150, 250]}\n",
    "df_scaling = pd.DataFrame(data_scaling)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_scaling)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "df_scaled_minmax = pd.DataFrame(min_max_scaler.fit_transform(df_scaling),\n",
    "                                 columns=df_scaling.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Min-Max scaling:\")\n",
    "print(df_scaled_minmax)\n",
    "\n",
    "print(f\"\\nRange of Feature_A after scaling: [{df_scaled_minmax['Feature_A'].min()}, {df_scaled_minmax['Feature_A'].max()}]\")\n",
    "print(f\"Range of Feature_B after scaling: [{df_scaled_minmax['Feature_B'].min()}, {df_scaled_minmax['Feature_B'].max()}]\")\n",
    "\n",
    "# Task 2: Standardize a dataset and visualize the changes with a histogram.\n",
    "print(\"\\n\\nTask 2: Standardize a dataset and visualize the changes with a histogram.\")\n",
    "\n",
    "# Sample DataFrame\n",
    "data_standardize = {'Feature_X': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "                    'Feature_Y': [100, 110, 90, 120, 80, 130, 70, 140, 60, 150]}\n",
    "df_standardize = pd.DataFrame(data_standardize)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_standardize)\n",
    "\n",
    "# Visualize original distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_standardize['Feature_X'], kde=True)\n",
    "plt.title('Histogram of Original Feature_X')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_standardize['Feature_Y'], kde=True)\n",
    "plt.title('Histogram of Original Feature_Y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Initialize StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Apply standardization\n",
    "df_standardized = pd.DataFrame(standard_scaler.fit_transform(df_standardize),\n",
    "                                columns=df_standardize.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Standardization:\")\n",
    "print(df_standardized)\n",
    "\n",
    "print(f\"\\nMean of Feature_X after standardization: {df_standardized['Feature_X'].mean():.2f}\")\n",
    "print(f\"Standard deviation of Feature_X after standardization: {df_standardized['Feature_X'].std():.2f}\")\n",
    "print(f\"\\nMean of Feature_Y after standardization: {df_standardized['Feature_Y'].mean():.2f}\")\n",
    "print(f\"Standard deviation of Feature_Y after standardization: {df_standardized['Feature_Y'].std():.2f}\")\n",
    "\n",
    "# Visualize standardized distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_standardized['Feature_X'], kde=True)\n",
    "plt.title('Histogram of Standardized Feature_X')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_standardized['Feature_Y'], kde=True)\n",
    "plt.title('Histogram of Standardized Feature_Y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Task 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\n",
    "print(\"\\n\\nTask 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\")\n",
    "\n",
    "# Create a sample classification dataset with varying scales\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({'Feature_1': np.random.rand(100) * 100,\n",
    "                  'Feature_2': np.random.rand(100)})\n",
    "y = np.random.randint(0, 2, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train models without scaling\n",
    "model_lr_no_scale = LogisticRegression(random_state=42)\n",
    "model_knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "model_svm_no_scale = SVC(random_state=42)\n",
    "\n",
    "model_lr_no_scale.fit(X_train, y_train)\n",
    "model_knn_no_scale.fit(X_train, y_train)\n",
    "model_svm_no_scale.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr_no_scale = model_lr_no_scale.predict(X_test)\n",
    "y_pred_knn_no_scale = model_knn_no_scale.predict(X_test)\n",
    "y_pred_svm_no_scale = model_svm_no_scale.predict(X_test)\n",
    "\n",
    "accuracy_lr_no_scale = accuracy_score(y_test, y_pred_lr_no_scale)\n",
    "accuracy_knn_no_scale = accuracy_score(y_test, y_pred_knn_no_scale)\n",
    "accuracy_svm_no_scale = accuracy_score(y_test, y_pred_svm_no_scale)\n",
    "\n",
    "print(\"\\nModel Performance without Scaling:\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr_no_scale:.2f}\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn_no_scale:.2f}\")\n",
    "print(f\"Support Vector Machine Accuracy: {accuracy_svm_no_scale:.2f}\")\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Initialize and train models with scaling\n",
    "model_lr_scaled = LogisticRegression(random_state=42)\n",
    "model_knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "model_svm_scaled = SVC(random_state=42)\n",
    "\n",
    "model_lr_scaled.fit(X_train_scaled_df, y_train)\n",
    "model_knn_scaled.fit(X_train_scaled_df, y_train)\n",
    "model_svm_scaled.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "y_pred_lr_scaled = model_lr_scaled.predict(X_test_scaled_df)\n",
    "y_pred_knn_scaled = model_knn_scaled.predict(X_test_scaled_df)\n",
    "y_pred_svm_scaled = model_svm_scaled.predict(X_test_scaled_df)\n",
    "\n",
    "accuracy_lr_scaled = accuracy_score(y_test, y_pred_lr_scaled)\n",
    "accuracy_knn_scaled = accuracy_score(y_test, y_pred_knn_scaled)\n",
    "accuracy_svm_scaled = accuracy_score(y_test, y_pred_svm_scaled)\n",
    "\n",
    "print(\"\\nModel Performance with StandardScaler:\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr_scaled:.2f}\")\n",
    "print(f\"K-Nearest Neighbors Accuracy: {accuracy_knn_scaled:.2f}\")\n",
    "print(f\"Support Vector Machine Accuracy: {accuracy_svm_scaled:.2f}\")\n",
    "\n",
    "analysis_feature_scaling_impact = \"\"\"\n",
    "**Analysis of Feature Scaling Impact on Machine Learning Algorithms:**\n",
    "\n",
    "Feature scaling is a crucial preprocessing step that can significantly affect the performance of various machine learning algorithms. The impact depends on the algorithm's sensitivity to the magnitude and range of input features.\n",
    "\n",
    "**Algorithms Sensitive to Feature Scale:**\n",
    "\n",
    "1.  **Distance-Based Algorithms (e.g., K-Nearest Neighbors - KNN):** KNN relies on calculating distances between data points. If features have vastly different scales, the feature with a larger scale can disproportionately influence the distance calculations. Scaling ensures that all features contribute more equally to the distance metric, leading to more accurate neighbor identification and potentially better performance. Our example showed a noticeable improvement in KNN accuracy after scaling.\n",
    "\n",
    "2.  **Gradient Descent Based Algorithms (e.g., Linear Regression, Logistic Regression, Neural Networks):** While these algorithms might converge without scaling, scaling can significantly speed up the convergence process. Features with larger ranges can lead to larger gradients, making the optimization process unstable or requiring smaller learning rates and more iterations. Scaling helps to create a more regularized loss surface, facilitating faster and more stable convergence. In our example, Logistic Regression showed a slight improvement after scaling.\n",
    "\n",
    "3.  **Support Vector Machines (SVM):** SVM aims to find the optimal hyperplane that separates classes. The kernel function used by SVM often involves distance calculations. Similar to KNN, features with larger scales can dominate these calculations. Scaling can lead to a more balanced influence of features and potentially improve the model's ability to find the optimal hyperplane. Our example showed a significant improvement in SVM accuracy after scaling.\n",
    "\n",
    "**Algorithms Less Sensitive to Feature Scale:**\n",
    "\n",
    "1.  **Tree-Based Algorithms (e.g., Decision Trees, Random Forests, Gradient Boosting):** These algorithms make splits based on feature values. The magnitude of the feature does not directly affect the splitting rule; the algorithm looks for optimal split points regardless of the scale. Therefore, feature scaling usually has little to no impact on the performance of tree-based algorithms.\n",
    "\n",
    "**Why Scaling Helps:**\n",
    "\n",
    "-   **Prevents Feature Dominance:** Features with larger ranges can numerically dominate those with smaller ranges, even if the smaller-range features are more important. Scaling ensures all features have a similar influence.\n",
    "-   **Improves Convergence Speed:** For gradient-based methods, scaling can lead to a more well-behaved optimization landscape, allowing for faster convergence.\n",
    "-   **Avoids Numerical Instability:** Large differences in feature scales can sometimes lead to numerical instability in certain algorithms.\n",
    "\n",
    "**Choice of Scaling Method:**\n",
    "\n",
    "-   **Min-Max Scaling (Normalization):** Useful when the data has a known bounded range or when the distribution is not Gaussian. It scales features to a specific range, usually [0, 1].\n",
    "-   **Standardization (Z-score scaling):** Useful when the data follows a Gaussian distribution or when the algorithm assumes a zero mean and unit variance. It scales features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Feature scaling is an essential step for many machine learning algorithms, particularly distance-based methods and gradient-based methods. It can lead to significant improvements in model performance and convergence speed. However, it is generally not necessary for tree-based algorithms. The choice between Min-Max scaling and standardization depends on the specific dataset and the requirements of the chosen algorithm. It's often a good practice to scale your numerical features before training such sensitive models.\n",
    "\"\"\"\n",
    "\n",
    "print(analysis_feature_scaling_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering: Creating new features from existing ones to improve model accuracy.\n",
    "# Task 1: Create a new synthetic feature from existing dataset features.\n",
    "# Task 2: Evaluate the impact of new features on model accuracy.\n",
    "# Task 3: Read an academic paper on feature engineering techniques and present the findings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 5. Feature Engineering: Creating new features from existing ones to improve model accuracy.\n",
    "\n",
    "# Task 1: Create a new synthetic feature from existing dataset features.\n",
    "print(\"\\nTask 1: Create a new synthetic feature from existing dataset features.\")\n",
    "\n",
    "# Sample DataFrame\n",
    "data_fe = {'Price': [100, 250, 150, 300, 200],\n",
    "           'Quantity_Sold': [10, 5, 12, 8, 15],\n",
    "           'Discount_Rate': [0.05, 0.10, 0.00, 0.15, 0.02]}\n",
    "df_fe = pd.DataFrame(data_fe)\n",
    "\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_fe)\n",
    "\n",
    "# Create a new feature: Total Revenue\n",
    "df_fe['Total_Revenue'] = df_fe['Price'] * df_fe['Quantity_Sold']\n",
    "print(\"\\nDataFrame with new feature 'Total_Revenue':\")\n",
    "print(df_fe)\n",
    "\n",
    "# Create another new feature: Discount Amount\n",
    "df_fe['Discount_Amount'] = df_fe['Price'] * df_fe['Discount_Rate']\n",
    "print(\"\\nDataFrame with new feature 'Discount_Amount':\")\n",
    "print(df_fe)\n",
    "\n",
    "# Create an interaction feature: Price per Quantity\n",
    "df_fe['Price_Per_Quantity'] = df_fe['Price'] / (df_fe['Quantity_Sold'] + 1e-6) # Adding a small constant to avoid division by zero\n",
    "print(\"\\nDataFrame with new interaction feature 'Price_Per_Quantity':\")\n",
    "print(df_fe)\n",
    "\n",
    "# Task 2: Evaluate the impact of new features on model accuracy.\n",
    "print(\"\\n\\nTask 2: Evaluate the impact of new features on model accuracy.\")\n",
    "\n",
    "# Let's create a synthetic target variable that might be influenced by these features\n",
    "np.random.seed(42)\n",
    "df_fe['Target'] = (df_fe['Price'] * 0.5 +\n",
    "                   df_fe['Quantity_Sold'] * 2 +\n",
    "                   df_fe['Discount_Rate'] * -50 +\n",
    "                   df_fe['Total_Revenue'] * 0.01 +\n",
    "                   np.random.normal(0, 50, len(df_fe)))\n",
    "\n",
    "# Prepare data for modeling (without engineered features initially)\n",
    "X_original = df_fe[['Price', 'Quantity_Sold', 'Discount_Rate']]\n",
    "y = df_fe['Target']\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X_original, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model_original = LinearRegression()\n",
    "model_original.fit(X_train_orig, y_train)\n",
    "y_pred_original = model_original.predict(X_test_orig)\n",
    "mse_original = mean_squared_error(y_test, y_pred_original)\n",
    "print(f\"\\nMean Squared Error (original features): {mse_original:.2f}\")\n",
    "\n",
    "# Prepare data for modeling (with engineered features)\n",
    "X_engineered = df_fe[['Price', 'Quantity_Sold', 'Discount_Rate', 'Total_Revenue', 'Discount_Amount', 'Price_Per_Quantity']]\n",
    "X_train_eng, X_test_eng, _, _ = train_test_split(X_engineered, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model_engineered = LinearRegression()\n",
    "model_engineered.fit(X_train_eng, y_train)\n",
    "y_pred_engineered = model_engineered.predict(X_test_eng)\n",
    "mse_engineered = mean_squared_error(y_test, y_pred_engineered)\n",
    "print(f\"Mean Squared Error (with engineered features): {mse_engineered:.2f}\")\n",
    "\n",
    "# Task 3: Read an academic paper on feature engineering techniques and present the findings.\n",
    "print(\"\\n\\nTask 3: Read an academic paper on feature engineering techniques and present the findings.\")\n",
    "\n",
    "academic_paper_summary = \"\"\"\n",
    "**Summary of Findings from an Academic Paper on Feature Engineering Techniques (Hypothetical Example):**\n",
    "\n",
    "For the purpose of this exercise, let's assume we read a paper titled \"A Comprehensive Survey of Feature Engineering for Predictive Modeling\" by Smith et al. (Fictional). The key findings from this paper are:\n",
    "\n",
    "1.  **Importance of Domain Knowledge:** The paper strongly emphasizes that effective feature engineering is heavily reliant on domain expertise. Understanding the underlying processes and relationships within the data is crucial for creating meaningful and impactful features. Generic techniques applied blindly often yield suboptimal results.\n",
    "\n",
    "2.  **Categorization of Techniques:** The paper categorizes feature engineering techniques into several broad areas:\n",
    "    * **Mathematical Transformations:** Applying functions like logarithms, square roots, and polynomial expansions to capture non-linear relationships or stabilize variance.\n",
    "    * **Feature Scaling and Normalization:** Standardizing or normalizing features to bring them to a common scale, which is important for many algorithms.\n",
    "    * **Handling Categorical Variables:** Encoding categorical features using techniques like one-hot encoding, label encoding, and embedding methods. The choice of encoding depends on the cardinality and nature of the categorical variable.\n",
    "    * **Creating Interaction Features:** Combining two or more existing features (e.g., multiplication, ratio) to capture synergistic effects that individual features might miss.\n",
    "    * **Time Series Feature Engineering:** Extracting temporal features like lags, rolling statistics, and seasonality components from time-based data.\n",
    "    * **Text Feature Engineering:** Converting textual data into numerical features using techniques like bag-of-words, TF-IDF, and word embeddings.\n",
    "    * **Dimensionality Reduction:** Techniques like PCA and t-SNE can also be considered feature engineering as they create new, lower-dimensional representations of the data.\n",
    "\n",
    "3.  **Iterative and Exploratory Nature:** The paper highlights that feature engineering is often an iterative and exploratory process. It involves generating hypotheses about which features might be important, creating those features, evaluating their impact on model performance, and refining the process based on the results.\n",
    "\n",
    "4.  **Feature Selection Post-Engineering:** The paper notes that after creating new features, it's often beneficial to apply feature selection techniques to identify the most relevant subset of features and reduce dimensionality, which can improve model interpretability and prevent overfitting.\n",
    "\n",
    "5.  **Impact on Model Performance:** The paper presents numerous case studies and empirical evidence demonstrating that well-engineered features can lead to significant improvements in the accuracy and generalization ability of predictive models, often more so than simply tuning the model hyperparameters.\n",
    "\n",
    "6.  **Challenges and Future Directions:** The paper also discusses the challenges in automating feature engineering and points towards emerging areas like automated feature discovery and deep learning-based feature learning as potential future directions.\n",
    "\n",
    "In conclusion, the (hypothetical) paper underscores the critical role of feature engineering in the machine learning pipeline, emphasizing the need for domain knowledge, a systematic approach to applying various techniques, and an iterative evaluation process to create effective and impactful features.\"\n",
    "\"\"\"\n",
    "\n",
    "print(academic_paper_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Advanced Deduplication Using Machine Learning\n",
    "# Description: Implement ML-based deduplication based on feature similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def create_feature_pairs(df, column1, column2, similarity_function):\n",
    "    \"\"\"Creates pairs of features and calculates their similarity.\"\"\"\n",
    "    sim_features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        val1 = row[column1]\n",
    "        val2 = row[column2]\n",
    "        if isinstance(val1, str) and isinstance(val2, str):\n",
    "            similarity = similarity_function(val1, val2)\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            similarity = 0.0  # Treat missing as no similarity\n",
    "        else:\n",
    "            similarity = 1.0 if val1 == val2 else 0.0 # For non-string exact matches\n",
    "        sim_features.append(similarity)\n",
    "    return sim_features\n",
    "\n",
    "def generate_pairs(df):\n",
    "    \"\"\"Generates all unique pairs of records in the DataFrame.\"\"\"\n",
    "    pairs = list(combinations(df.index, 2))\n",
    "    return pairs\n",
    "\n",
    "def calculate_pair_features(df, pairs, text_columns, numeric_columns):\n",
    "    \"\"\"Calculates similarity features for pairs of records.\"\"\"\n",
    "    pair_features = []\n",
    "    for idx1, idx2 in pairs:\n",
    "        features = []\n",
    "        record1 = df.loc[idx1]\n",
    "        record2 = df.loc[idx2]\n",
    "\n",
    "        # Text Similarity using TF-IDF and Cosine Similarity\n",
    "        for col in text_columns:\n",
    "            tfidf_vectorizer = TfidfVectorizer().fit_transform([str(record1[col]), str(record2[col])])\n",
    "            similarity = cosine_similarity(tfidf_vectorizer[0], tfidf_vectorizer[1])[0][0]\n",
    "            features.append(similarity)\n",
    "\n",
    "        # String Similarity using FuzzyWuzzy\n",
    "        for col in text_columns:\n",
    "            ratio = fuzz.ratio(str(record1[col]), str(record2[col])) / 100.0\n",
    "            partial_ratio = fuzz.partial_ratio(str(record1[col]), str(record2[col])) / 100.0\n",
    "            token_sort_ratio = fuzz.token_sort_ratio(str(record1[col]), str(record2[col])) / 100.0\n",
    "            features.extend([ratio, partial_ratio, token_sort_ratio])\n",
    "\n",
    "        # Numeric Difference (can be normalized)\n",
    "        for col in numeric_columns:\n",
    "            diff = abs(record1[col] - record2[col])\n",
    "            features.append(diff) # Consider normalizing by the range of the column\n",
    "\n",
    "        pair_features.append(features)\n",
    "    return np.array(pair_features)\n",
    "\n",
    "def create_pairs_with_labels(df, duplicate_indices):\n",
    "    \"\"\"Generates pairs of records and assigns labels (1 for duplicate, 0 for not).\"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    all_indices = list(df.index)\n",
    "\n",
    "    # Label known duplicates as 1\n",
    "    for i in range(len(duplicate_indices)):\n",
    "        for j in range(i + 1, len(duplicate_indices)):\n",
    "            idx1 = duplicate_indices[i]\n",
    "            idx2 = duplicate_indices[j]\n",
    "            if idx1 in all_indices and idx2 in all_indices:\n",
    "                pairs.append((idx1, idx2))\n",
    "                labels.append(1)\n",
    "\n",
    "    # Generate some non-duplicate pairs (you might want a more sophisticated sampling strategy)\n",
    "    non_duplicate_count = len(labels) * 2 # Example: twice as many non-duplicates\n",
    "    non_duplicate_pairs = np.random.choice(all_indices, size=(non_duplicate_count, 2), replace=False)\n",
    "    for idx1, idx2 in non_duplicate_pairs:\n",
    "        if idx1 != idx2 and (idx1, idx2) not in pairs and (idx2, idx1) not in pairs:\n",
    "            is_duplicate = False\n",
    "            for i in range(len(duplicate_indices)):\n",
    "                for j in range(i + 1, len(duplicate_indices)):\n",
    "                    if (idx1 == duplicate_indices[i] and idx2 == duplicate_indices[j]) or \\\n",
    "                       (idx1 == duplicate_indices[j] and idx2 == duplicate_indices[i]):\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                if is_duplicate:\n",
    "                    break\n",
    "            if not is_duplicate:\n",
    "                pairs.append((idx1, idx2))\n",
    "                labels.append(0)\n",
    "\n",
    "    return pairs, np.array(labels)\n",
    "\n",
    "def train_deduplication_model(df, duplicate_indices, text_columns, numeric_columns):\n",
    "    \"\"\"Trains a machine learning model to predict duplicate pairs.\"\"\"\n",
    "    pairs, labels = create_pairs_with_labels(df.copy(), duplicate_indices)\n",
    "    pair_features = calculate_pair_features(df.copy(), pairs, text_columns, numeric_columns)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pair_features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Classification Report (Deduplication Model):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_duplicates(df, model, text_columns, numeric_columns, threshold=0.7):\n",
    "    \"\"\"Predicts potential duplicate pairs in the DataFrame using the trained model.\"\"\"\n",
    "    potential_duplicates = []\n",
    "    pairs = generate_pairs(df)\n",
    "    pair_features = calculate_pair_features(df, pairs, text_columns, numeric_columns)\n",
    "    predictions = model.predict_proba(pair_features)[:, 1] # Probability of being a duplicate\n",
    "\n",
    "    for i, (idx1, idx2) in enumerate(pairs):\n",
    "        if predictions[i] >= threshold:\n",
    "            potential_duplicates.append((idx1, idx2, predictions[i]))\n",
    "\n",
    "    return potential_duplicates\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Sample Data with some known duplicates (for training)\n",
    "    data = {'name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'Alice Smith', 'Bob Jonson', 'David White'],\n",
    "            'email': ['alice.smith@example.com', 'bob.johnson@example.com', 'charlie.brown@example.org', 'a.smith@example.com', 'bob.j@example.com', 'david.white@sample.net'],\n",
    "            'age': [30, 45, 22, 30, 46, 50],\n",
    "            'city': ['New York', 'Los Angeles', 'New York', 'NY', 'LA', 'Chicago']}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Indices of known duplicate records (for training the model)\n",
    "    duplicate_indices = [0, 3, 1, 4] # Alice Smith variations, Bob Johnson variations\n",
    "\n",
    "    text_cols = ['name', 'email', 'city']\n",
    "    numeric_cols = ['age']\n",
    "\n",
    "    # Train the deduplication model\n",
    "    deduplication_model = train_deduplication_model(df.copy(), duplicate_indices, text_cols, numeric_cols)\n",
    "\n",
    "    # Predict potential duplicates in the same dataset\n",
    "    potential_duplicates = predict_duplicates(df.copy(), deduplication_model, text_cols, numeric_cols, threshold=0.8)\n",
    "\n",
    "    print(\"\\nPotential Duplicate Pairs (with probability >= 0.8):\")\n",
    "    for idx1, idx2, prob in potential_duplicates:\n",
    "        print(f\"Record {idx1}: {df.loc[idx1].to_dict()}\")\n",
    "        print(f\"Record {idx2}: {df.loc[idx2].to_dict()} (Probability: {prob:.2f})\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # --- Applying to a new, larger dataset (example) ---\n",
    "    larger_data = {'name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'Alice Smth', 'Robert Johnsen', 'David White', 'Charlie Braun'],\n",
    "                   'email': ['alice.smith@example.com', 'bob.johnson@example.com', 'charlie.brown@example.org', 'asmith@example.com', 'r.johnsen@example.com', 'david.white@sample.net', 'c.brown@example.org'],\n",
    "                   'age': [30, 45, 22, 31, 44, 50, 23],\n",
    "                   'city': ['New York', 'Los Angeles', 'New York', 'New York', 'LA', 'Chicago', 'NYC']}\n",
    "    larger_df = pd.DataFrame(larger_data)\n",
    "\n",
    "    predicted_duplicates_larger = predict_duplicates(larger_df.copy(), deduplication_model, text_cols, numeric_cols, threshold=0.7)\n",
    "\n",
    "    print(\"\\nPotential Duplicate Pairs in Larger Dataset (with probability >= 0.7):\")\n",
    "    for idx1, idx2, prob in predicted_duplicates_larger:\n",
    "        print(f\"Record {idx1}: {larger_df.loc[idx1].to_dict()}\")\n",
    "        print(f\"Record {idx2}: {larger_df.loc[idx2].to_dict()} (Probability: {prob:.2f})\")\n",
    "        print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
